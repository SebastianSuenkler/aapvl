%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
 \ifdefined\DeclareUnicodeCharacterAsOptional
  \DeclareUnicodeCharacter{"00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{"2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{"2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{"2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{"251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{"2572}{\textbackslash}
 \else
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{2572}{\textbackslash}
 \fi
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}



\title{AAPVL Documentation}
\date{Mar 28, 2018}
\release{0.1}
\author{Dorle Osterode}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{index::doc}}



\chapter{Installation and Requirements}
\label{\detokenize{installation::doc}}\label{\detokenize{installation:installation-and-requirements}}\label{\detokenize{installation:welcome-to-aapvl-s-documentation}}\label{\detokenize{installation:installation}}
This chapter gives a short overview how to install the backend of the
AAPVL-Project and on which other projects this program relies on.


\section{How to install the program:}
\label{\detokenize{installation:how-to-install-the-program}}
First you should make sure, that all the dependencies are
installed. If you install some dependencies manually, make sure they
are in the proper path, so they can be found.

When all dependencies are installed you only need to clone the
git-repository TODO: url here and everything should work.


\section{List of dependencies:}
\label{\detokenize{installation:list-of-dependencies}}

\subsection{Python packages:}
\label{\detokenize{installation:python-packages}}
These packages can be installed through the package manager of your
distro or through the python package manager pip.
\begin{itemize}
\item {} 
numpy

\item {} 
scipy

\item {} 
sklearn (version 0.18)

\item {} 
mysqldb (version 1.2.3. you need to install some extra packages
(libmysqlclient-dev and python-dev) for that. instead of using pip
you can install it directly with \sphinxcode{\sphinxupquote{aptitude install
python-mysqldb}})

\item {} 
opencv and python-opencv (you should use the official packages here:
\sphinxcode{\sphinxupquote{aptitude install opencv python-opencv}})

\item {} 
nltk

\item {} 
sklearn-crfsuite

\item {} 
bs4

\item {} 
dateutil

\item {} 
sphinx (to build the documentation only. If you use virtual
environments, sphinx has to be installed in the same virtual
enviroment to be able to build the api reference.)

\end{itemize}


\subsection{non-standard libraries and programs:}
\label{\detokenize{installation:non-standard-libraries-and-programs}}
For installation of the non-standard libraries and programs please
follow the installation guides for that program.
\begin{itemize}
\item {} 
libpostal with python bindings (package name \sphinxcode{\sphinxupquote{postal}}):
\sphinxurl{https://github.com/openvenues/libpostal}

\item {} 
ParZu: \sphinxurl{https://github.com/rsennrich/ParZu}

\item {} 
Zmorge model for ParZu: \sphinxurl{http://kitt.ifi.uzh.ch/kitt/zmorge/}

\end{itemize}

Some notes to both programs:


\subsection{libpostal:}
\label{\detokenize{installation:libpostal}}
If you install libpostal into a user specific path and you want to
install the python package postal afterwards, you have to specify the
path to libpostall for the installation. With pip you can use:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{k}{global}\PYG{o}{\PYGZhy{}}\PYG{n}{option}\PYG{o}{=}\PYG{n}{build\PYGZus{}ext} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{k}{global}\PYG{o}{\PYGZhy{}}\PYG{n}{option}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}L/home/foo/libs/libpostal\PYGZus{}build/lib}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{k}{global}\PYG{o}{\PYGZhy{}}\PYG{n}{option}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}I/home/foo/libs/libpostal\PYGZus{}build/include}\PYG{l+s+s2}{\PYGZdq{}} \PYG{n}{postal}
\end{sphinxVerbatim}


\subsection{ParZu:}
\label{\detokenize{installation:parzu}}
It is sufficient to follow the installation instructions for ParZu
until step 3 first part. You don’t have to use the script
\sphinxcode{\sphinxupquote{install.sh}}. All the components installed with this script are
not used from this program.


\chapter{First steps with the program}
\label{\detokenize{tutorial:first-steps-with-the-program}}\label{\detokenize{tutorial::doc}}
This chapter gives a short overview and some simple examples how to
use the program. With the general flags \sphinxcode{\sphinxupquote{config}} and
\sphinxcode{\sphinxupquote{debug}} can be used to provide another config file or to get
more information about each module in the log. In the following
examples they are omitted.


\section{Testing the setup:}
\label{\detokenize{tutorial:testing-the-setup}}
To test the proper setup of all libraries, models and external data
files, you can use the simple test. You can invoke this test as
follows:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{src}\PYG{o}{/}\PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{test} \PYG{n}{simple}
\end{sphinxVerbatim}

Some example files are loaded to the database and all modules are run
on every file.


\section{Invoking the backend:}
\label{\detokenize{tutorial:invoking-the-backend}}
You can invoke the backend, so that every job, that is added to the
database, will be processed.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{src}\PYG{o}{/}\PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{run}
\end{sphinxVerbatim}

In the configuration you can choose how often the backend should look
for jobs in the database and when the backend should shut itself down.


\section{Training a classifier:}
\label{\detokenize{tutorial:training-a-classifier}}
You can retrain a single classifier, e.g. the shop classifier, like
this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{src}\PYG{o}{/}\PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{train} \PYG{n}{shop} \PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{data}
\end{sphinxVerbatim}

The directory \sphinxcode{\sphinxupquote{path/to/data}} has to contain two subdirectories
\sphinxcode{\sphinxupquote{0}} and \sphinxcode{\sphinxupquote{1}}, which contain the single data
files. \sphinxcode{\sphinxupquote{0}} is interpreted as the positive class and \sphinxcode{\sphinxupquote{1}} as
the negative class. See chapter {\hyperref[\detokenize{training:training}]{\sphinxcrossref{\DUrole{std,std-ref}{How to train and test the single classifiers}}}} for more information.


\section{Training a crf:}
\label{\detokenize{tutorial:training-a-crf}}
Two modules use conditional random fields. You can train these two
conditional random fields, e.g. the one used to extract addresses,
like this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{src}\PYG{o}{/}\PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{train} \PYG{n}{imp} \PYG{n}{addresses}\PYG{o}{.}\PYG{n}{txt} \PYG{n}{labels}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}

The file \sphinxcode{\sphinxupquote{addresses.txt}} contains one address per line and in
\sphinxcode{\sphinxupquote{labels.txt}} in each line a label for each word is
contained. See chapter {\hyperref[\detokenize{training:training}]{\sphinxcrossref{\DUrole{std,std-ref}{How to train and test the single classifiers}}}} for more information.


\section{Testing a classifier:}
\label{\detokenize{tutorial:testing-a-classifier}}
To test one of the classifiers (using conditional random fields or
support vector machines), you can use:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{src}\PYG{o}{/}\PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{test} \PYG{n}{shop} \PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{data}
\end{sphinxVerbatim}


\section{Updating a classifier:}
\label{\detokenize{tutorial:updating-a-classifier}}
When you obtain additional data and want to update one of the suport
vector machine using classifiers, you can use this subcommand to
update the classifier directly with data from a directory.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{src}\PYG{o}{/}\PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{update} \PYG{n}{shop} \PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{data}
\end{sphinxVerbatim}

The directory has to conform with the assumptions. See chapter
{\hyperref[\detokenize{training:training}]{\sphinxcrossref{\DUrole{std,std-ref}{How to train and test the single classifiers}}}} for more information.


\section{Loading data in db:}
\label{\detokenize{tutorial:loading-data-in-db}}
To test not only the setup but to test a broder spectrum you can load
some files into the database with a specific selection of modules
registered for these files. With

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{src}\PYG{o}{/}\PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{load} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{modules} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{1,2,3}\PYG{l+s+s2}{\PYGZdq{}} \PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{data}
\end{sphinxVerbatim}

you load all files in \sphinxcode{\sphinxupquote{path/to/data}} in the database and
register the modules 1, 2 and 3 for them. With running

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{src}\PYG{o}{/}\PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{run}
\end{sphinxVerbatim}

you can process these files. Note that this is just for testing
purposes and some functionality, like e.g. the information summary for
all subpages, is not available here.


\chapter{How to configure the program}
\label{\detokenize{configuration:configuration}}\label{\detokenize{configuration::doc}}\label{\detokenize{configuration:how-to-configure-the-program}}
The program has some parameters (e.g. user name for the database
connection etc.) that should be configured. Therefore a default
configuration file (short: config file) exists and can be
customized. The different parameters and their meaning is explained in
this chapter and the default config file is shown as example. Note:
the keywords are case sensitiv, so make sure you spell them correctly.


\section{Configuration parameters:}
\label{\detokenize{configuration:configuration-parameters}}\begin{description}
\item[{host}] \leavevmode
IP adress to which the database is reachable. You can leave the
default value, when you have your database local on your
machine. default: 127.0.0.1

\item[{user}] \leavevmode
Username for the database user. default: foo

\item[{passwd}] \leavevmode
Password for the database user. default: bar

\item[{db}] \leavevmode
Name of the used database. default: aapvl

\item[{max\_tries}] \leavevmode
Maximum number of failed tries to obtain jobs from the database before
ending the program. If this number is negative, the program tries four
times per day to obtain jobs and then sleeps in the background for the
rest of the time. default: 3

\item[{delay}] \leavevmode
Time in seconds to sleep before trying to get jobs from the
database. The delay time is only used, after a try to obtain jobs was
not succesful and is not used when max\_tries is negative (see also
\sphinxstylestrong{max\_tries}). default: 3600

\item[{delay\_module}] \leavevmode
Time in seconds after which each module has to finished. If a module
needs more than delay\_module seconds, it is terminated with all its
subprocesses and None is returned as a result from this
module. default: 3600

\item[{day}] \leavevmode
Number of times per day the program should wakeup and try to obtain
jobs from the database. default: 4

\item[{update\_rate}] \leavevmode
Number of days after which the database should be checked for
altered classification results for online learning. default: 7

\item[{food\_vocab}] \leavevmode
Path to a file containing food relevant vocabulary for the food
classifier (see also chapter {\hyperref[\detokenize{external_data:external-food-vocab}]{\sphinxcrossref{\DUrole{std,std-ref}{The food vocabulary (module 3):}}}}). default:
data/food\_vocab.txt

\item[{map\_file}] \leavevmode
Path to a file containing an assignment between postal codes and
german regions. For more details also information on the assumed
format see chapter {\hyperref[\detokenize{external_data:external-postal-code}]{\sphinxcrossref{\DUrole{std,std-ref}{Postal codes for address extraction (module 1):}}}}. default:
data/plz.csv

\item[{legal\_numbers}] \leavevmode
Path to a file containing all approved boards of control for
ecological traders. For more information on the file and format see
also chapter {\hyperref[\detokenize{external_data:external-legal-numbers}]{\sphinxcrossref{\DUrole{std,std-ref}{Legal numbers for ecological control posts (module 6):}}}}. default:
data/legal\_oeko\_numbers.txt

\item[{health\_claim\_substances}] \leavevmode
Path to a file with common substances used in health claims. For
more information on the file, the format and how to update it see
also chapter {\hyperref[\detokenize{external_data:external-hc-substances}]{\sphinxcrossref{\DUrole{std,std-ref}{List with substances (module 7):}}}} and
{\hyperref[\detokenize{online_learning:substances-online}]{\sphinxcrossref{\DUrole{std,std-ref}{List with substances:}}}}. default:
data/health\_claim\_substances.txt

\item[{health\_claim\_diseases}] \leavevmode
Path to a file with common diseases used in health claims. For more
information on the file, the format and how to update it see also
chapter {\hyperref[\detokenize{external_data:external-hc-disease}]{\sphinxcrossref{\DUrole{std,std-ref}{List with disease (module 7):}}}} and
{\hyperref[\detokenize{online_learning:diseases-online}]{\sphinxcrossref{\DUrole{std,std-ref}{List with disease:}}}}. default: data/health\_claim\_diseases.txt

\item[{health\_claim\_rejected}] \leavevmode
Path to a file with rejected health claims. For more information on
the file, the format and how to update it see also chapter
{\hyperref[\detokenize{external_data:external-hc-rejected}]{\sphinxcrossref{\DUrole{std,std-ref}{List with rejected health claims (module 7):}}}} and {\hyperref[\detokenize{online_learning:rejected-online}]{\sphinxcrossref{\DUrole{std,std-ref}{List with rejected health claims:}}}}. default:
data/rejected\_health\_claim.txt

\item[{health\_claim\_declination}] \leavevmode
Path to a file with declinations of verbs probably used in health
claims. For more information on the file, the format and how to
update it see also chapter {\hyperref[\detokenize{external_data:external-hc-declination}]{\sphinxcrossref{\DUrole{std,std-ref}{List with verb declinations (module 7):}}}} and
{\hyperref[\detokenize{online_learning:declination-online}]{\sphinxcrossref{\DUrole{std,std-ref}{List with verb declinations:}}}}. default:
data/health\_claim\_declination.txt

\item[{door\_list}] \leavevmode
Path to a file with the eu door list. For more information on the
assumed format see chapter {\hyperref[\detokenize{external_data:external-door}]{\sphinxcrossref{\DUrole{std,std-ref}{EU door list (module 8):}}}}. default:
data/door\_list.csv

\item[{ingredients\_whitelist}] \leavevmode
Path to a file with known ingredients. For more information on the
format or how to update the list see chapter
{\hyperref[\detokenize{external_data:external-black-white}]{\sphinxcrossref{\DUrole{std,std-ref}{White- and blacklist for ingredients (module 9):}}}} and {\hyperref[\detokenize{online_learning:black-white-online}]{\sphinxcrossref{\DUrole{std,std-ref}{Adding information to the white- and blacklist for ingredients (module 9):}}}}. default:
data/whitelist.txt

\item[{ingredients\_blacklist}] \leavevmode
Path to a file with prohibited ingredients. For more information on
the format or how to update the list see chapter
{\hyperref[\detokenize{external_data:external-black-white}]{\sphinxcrossref{\DUrole{std,std-ref}{White- and blacklist for ingredients (module 9):}}}} and {\hyperref[\detokenize{online_learning:black-white-online}]{\sphinxcrossref{\DUrole{std,std-ref}{Adding information to the white- and blacklist for ingredients (module 9):}}}}. default:
data/blacklist.txt

\item[{parzu}] \leavevmode
Path to the parzu executable. See chapter {\hyperref[\detokenize{installation:installation}]{\sphinxcrossref{\DUrole{std,std-ref}{Installation and Requirements}}}} for more
information how to install parzu. default: /home/foo/ParZu/parzu

\end{description}


\section{Example:}
\label{\detokenize{configuration:example}}
The default config file:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{host} \PYG{o}{=} \PYG{l+m+mf}{127.0}\PYG{o}{.}\PYG{l+m+mf}{0.1}
\PYG{n}{user} \PYG{o}{=} \PYG{n}{foo}
\PYG{n}{passwd} \PYG{o}{=} \PYG{n}{bar}
\PYG{n}{db} \PYG{o}{=} \PYG{n}{aapvl}
\PYG{n}{max\PYGZus{}tries} \PYG{o}{=} \PYG{l+m+mi}{3}
\PYG{n}{delay} \PYG{o}{=} \PYG{l+m+mi}{3600}
\PYG{n}{delay\PYGZus{}module} \PYG{o}{=} \PYG{l+m+mi}{3600}
\PYG{n}{day} \PYG{o}{=} \PYG{l+m+mi}{4}
\PYG{n}{update\PYGZus{}rate} \PYG{o}{=} \PYG{l+m+mi}{7}
\PYG{n}{food\PYGZus{}vocab} \PYG{o}{=} \PYG{n}{data}\PYG{o}{/}\PYG{n}{food\PYGZus{}vocab}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{map\PYGZus{}file} \PYG{o}{=} \PYG{n}{data}\PYG{o}{/}\PYG{n}{plz}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{legal\PYGZus{}numbers} \PYG{o}{=} \PYG{n}{data}\PYG{o}{/}\PYG{n}{legal\PYGZus{}oeko\PYGZus{}numbers}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{health\PYGZus{}claim\PYGZus{}substances} \PYG{o}{=} \PYG{n}{data}\PYG{o}{/}\PYG{n}{health\PYGZus{}claim\PYGZus{}substances}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{health\PYGZus{}claim\PYGZus{}diseases} \PYG{o}{=} \PYG{n}{data}\PYG{o}{/}\PYG{n}{health\PYGZus{}claim\PYGZus{}diseases}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{health\PYGZus{}claim\PYGZus{}rejected} \PYG{o}{=} \PYG{n}{data}\PYG{o}{/}\PYG{n}{rejected\PYGZus{}health\PYGZus{}claim}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{health\PYGZus{}claim\PYGZus{}declination} \PYG{o}{=} \PYG{n}{data}\PYG{o}{/}\PYG{n}{health\PYGZus{}claim\PYGZus{}declination}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{door\PYGZus{}list} \PYG{o}{=} \PYG{n}{data}\PYG{o}{/}\PYG{n}{door\PYGZus{}list}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{ingredients\PYGZus{}whitelist} \PYG{o}{=} \PYG{n}{data}\PYG{o}{/}\PYG{n}{whitelist}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{ingredients\PYGZus{}blacklist} \PYG{o}{=} \PYG{n}{data}\PYG{o}{/}\PYG{n}{blacklist}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{parzu} \PYG{o}{=} \PYG{o}{/}\PYG{n}{home}\PYG{o}{/}\PYG{n}{foo}\PYG{o}{/}\PYG{n}{ParZu}\PYG{o}{/}\PYG{n}{parzu}
\end{sphinxVerbatim}


\chapter{External data}
\label{\detokenize{external_data:id1}}\label{\detokenize{external_data::doc}}\label{\detokenize{external_data:external-data}}
Several modules relie on data that is externally provided. This is
data, that changes probably often and should therefore be extandable
or exchangeable. There are two different formats in which the data is
assumed, either plaintext with a single phrase per line or in a csv
format. Each file is assumed to be utf-8 encoded.

For every module, that uses external data, the format is described in
the following sections. How to change the default location of one of
the files is described in chapter {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{How to configure the program}}}}. Furthermore a
description how to extend which files to obtain online learning for
some modules is described in chapter {\hyperref[\detokenize{online_learning:online}]{\sphinxcrossref{\DUrole{std,std-ref}{How to use online-learning with the program}}}}.


\section{Postal codes for address extraction (module 1):}
\label{\detokenize{external_data:external-postal-code}}\label{\detokenize{external_data:postal-codes-for-address-extraction-module-1}}
The module for the extraction of addresses assumes a csv formatted
file with an assignment from postal codes to regions (Bundesland and
Kreis). An example of the assumed format is given below:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{PLZ2}\PYG{p}{,}\PYG{n}{ZUST\PYGZus{}BUNDESLAND\PYGZus{}STAAT}\PYG{p}{,}\PYG{n}{KREIS}
\PYG{l+m+mi}{01067}\PYG{p}{,}\PYG{n}{Sachsen}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Dresden, Stadt}\PYG{l+s+s2}{\PYGZdq{}}
\end{sphinxVerbatim}

Note that the first line contains header information and is ignored
therefor. The corresponding key in the configuration file is
“map\_file” (see chapter {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{How to configure the program}}}}).


\section{The food vocabulary (module 3):}
\label{\detokenize{external_data:external-food-vocab}}\label{\detokenize{external_data:the-food-vocabulary-module-3}}
The food classifier (module 3) uses a fixed vocabulary to classify
text. The file is in plaintext format and contains one word per
line. An example is given below:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Apfel}
\PYG{n}{Banane}
\PYG{n}{Citrone}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

The corresponding key in the configuration file is “food\_vocab” (see
chapter {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{How to configure the program}}}}). In section {\hyperref[\detokenize{online_learning:food-vocab-label}]{\sphinxcrossref{\DUrole{std,std-ref}{Adding words to the food vocabulary (module 3):}}}} is
explained how you can add words to the vocabulary.


\section{Legal numbers for ecological control posts (module 6):}
\label{\detokenize{external_data:legal-numbers-for-ecological-control-posts-module-6}}\label{\detokenize{external_data:external-legal-numbers}}
The module for the verification of traders of ecological products uses
a list of valid german “Ökonummern”. These are numbers from control
posts, which control and certificate the traders. Because the control
posts change from time to time, this list should be always up to
date. The numbers are given in plaintext and an example is given
below:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{DE}\PYG{o}{\PYGZhy{}}\PYG{n}{ÖKO}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{001}
\PYG{n}{DE}\PYG{o}{\PYGZhy{}}\PYG{n}{ÖKO}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{003}
\end{sphinxVerbatim}

The corresponding key in the configuration file is “legal\_numbers”
(see chapter {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{How to configure the program}}}}).


\section{EU door list (module 8):}
\label{\detokenize{external_data:external-door}}\label{\detokenize{external_data:eu-door-list-module-8}}
The module for the validation of the use of specific product names
(restricted with PDO, PGI or TSG) uses a list with certified products
(door list). This list contains every product that is registered in
the EU for PDO, PGI or TSG and can be exported from here:
\sphinxurl{http://ec.europa.eu/agriculture/quality/door/list.html} The format is
csv and an example is given below:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}
\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}
\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}
\PYG{n}{Dossier} \PYG{n}{Number}  \PYG{p}{,}      \PYG{n}{Designation}        \PYG{p}{,} \PYG{n}{Country} \PYG{p}{,} \PYG{n}{ISO} \PYG{p}{,}   \PYG{n}{Status}   \PYG{p}{,}   \PYG{n}{Type}    \PYG{p}{,} \PYG{n}{Last} \PYG{n}{relevant} \PYG{n}{date} \PYG{p}{,}   \PYG{n}{Product} \PYG{n}{Categrory}   \PYG{p}{,}      \PYG{n}{Latin} \PYG{n}{Transcription}     \PYG{p}{,} \PYG{n}{Submission} \PYG{n}{date} \PYG{p}{,} \PYG{n}{Publication} \PYG{n}{date} \PYG{p}{,} \PYG{n}{Registration} \PYG{n}{date} \PYG{p}{,} \PYG{l+m+mi}{1}\PYG{n}{st} \PYG{n}{Amendment} \PYG{n}{date} \PYG{p}{,} \PYG{l+m+mi}{2}\PYG{n}{nd} \PYG{n}{Amendment} \PYG{n}{date} \PYG{p}{,} \PYG{l+m+mi}{3}\PYG{n}{rd} \PYG{n}{Amendment} \PYG{n}{date}
\PYG{n}{PL}\PYG{o}{/}\PYG{n}{PGI}\PYG{o}{/}\PYG{l+m+mi}{0005}\PYG{o}{/}\PYG{l+m+mi}{02154}\PYG{p}{,}\PYG{n}{Kiełbasa} \PYG{n}{piaszczańska}\PYG{p}{,}\PYG{n}{Poland}\PYG{p}{,}\PYG{n}{PL}\PYG{p}{,}\PYG{n}{Registered}\PYG{p}{,}\PYG{n}{PGI}\PYG{p}{,}\PYG{l+m+mi}{21}\PYG{o}{/}\PYG{l+m+mi}{11}\PYG{o}{/}\PYG{l+m+mi}{2017}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Class 1.2. Meat products (cooked, salted, smoked, etc.)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{o}{/}\PYG{l+m+mi}{07}\PYG{o}{/}\PYG{l+m+mi}{2016}\PYG{p}{,}\PYG{l+m+mi}{29}\PYG{o}{/}\PYG{l+m+mi}{06}\PYG{o}{/}\PYG{l+m+mi}{2017}\PYG{p}{,}\PYG{l+m+mi}{21}\PYG{o}{/}\PYG{l+m+mi}{11}\PYG{o}{/}\PYG{l+m+mi}{2017}\PYG{p}{,}\PYG{p}{,}\PYG{p}{,}
\end{sphinxVerbatim}

Note that the first four lines contain header or no information and
are ignored therefor. The corresponding key for the configuration
file is “door\_list” (see chapter {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{How to configure the program}}}}).


\section{White- and blacklist for ingredients (module 9):}
\label{\detokenize{external_data:white-and-blacklist-for-ingredients-module-9}}\label{\detokenize{external_data:external-black-white}}
To perform a validation of the found ingredients of a product a white-
and a blacklist are used. It is checked, if an ingredient is already
known and allowed (contained in the whitelist), already known and
prohibited (contained in the blacklist) or unknown. Therefor a white-
and a blacklist have to be provided. The assumed format is plaintext
and an example is given below:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Milch}
\PYG{n}{Zitronen}
\PYG{n}{flüssig}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

The corresponding keywords in the configuration file are
“ingredients\_whitelist” and “ingredients\_blacklist” (see chapter
{\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{How to configure the program}}}}). These files can be extended with words to
obtain some kind of online learning see section
{\hyperref[\detokenize{online_learning:black-white-online}]{\sphinxcrossref{\DUrole{std,std-ref}{Adding information to the white- and blacklist for ingredients (module 9):}}}} for more information.


\section{List with substances (module 7):}
\label{\detokenize{external_data:external-hc-substances}}\label{\detokenize{external_data:list-with-substances-module-7}}
For the detection of possible health claims a list with commonly used
substances in health claims should be provided. This list should
contain single words and phrases. The file is assumed to be in
plaintext and an example is given below:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Vitamin} \PYG{n}{C}
\PYG{n}{Eisen}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

The corresponding keyword in the configuration file is
“health\_claim\_substances” (see chapter {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{How to configure the program}}}}). This
file can be extended with words to obtain some kind of online learning
see section {\hyperref[\detokenize{online_learning:substances-online}]{\sphinxcrossref{\DUrole{std,std-ref}{List with substances:}}}} for more information.


\section{List with disease (module 7):}
\label{\detokenize{external_data:list-with-disease-module-7}}\label{\detokenize{external_data:external-hc-disease}}
For the detection of possible health claims a list with commonly used
diseases in health claims should be provided. This list should contain
single words and phrases. The file is assumed to be in plaintext and
an example is given below:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Herzinfarkt}
\PYG{n}{rote} \PYG{n}{Blutkörperchen}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

The corresponding keyword in the configuration file is
“health\_claim\_diseases” (see chapter {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{How to configure the program}}}}). This file
can be extended with words to obtain some kind of online learning see
section {\hyperref[\detokenize{online_learning:diseases-online}]{\sphinxcrossref{\DUrole{std,std-ref}{List with disease:}}}} for more information.


\section{List with verb declinations (module 7):}
\label{\detokenize{external_data:list-with-verb-declinations-module-7}}\label{\detokenize{external_data:external-hc-declination}}
To prefilter sentences after a semantic analysis of a given text, a
list with relevant verbs for health claims should be provided. In this
file all relevant declinations of the verb has to be listed. The
declinations for one verb should be delimited by a newline and between
two verbs there can be one additional newline. This file is assumed to
be in plaintext and an example is given below:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{beitragen}
\PYG{n}{trägt} \PYG{n}{bei}
\PYG{n}{tragen} \PYG{n}{bei}
\PYG{n}{trug} \PYG{n}{bei}
\PYG{n}{trugen} \PYG{n}{bei}
\PYG{n}{hat} \PYG{n}{beigetragen}
\PYG{n}{haben} \PYG{n}{beigetragen}
\PYG{n}{wird} \PYG{n}{beitragen}
\PYG{n}{werden} \PYG{n}{beitragen}

\PYG{n}{haben}
\PYG{n}{hat}
\PYG{n}{haben}
\PYG{n}{hatte}
\PYG{n}{hatten}
\end{sphinxVerbatim}

The corresponding keyword in the configuration file is
“health\_claim\_declination” (see chapter {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{How to configure the program}}}}). This
file can be extended with more verb declinations (see chapter
{\hyperref[\detokenize{online_learning:declination-online}]{\sphinxcrossref{\DUrole{std,std-ref}{List with verb declinations:}}}}).


\section{List with rejected health claims (module 7):}
\label{\detokenize{external_data:list-with-rejected-health-claims-module-7}}\label{\detokenize{external_data:external-hc-rejected}}
A list with rejected health claims should be provided to detect
resellers, that use exactly these health claims. The file is assumed
to be in plaintext and an example is given below:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Actimirell} \PYG{n}{aktiviert} \PYG{n}{Abwehkräfte}\PYG{o}{.}
\PYG{n}{Milchschneideling} \PYG{n}{macht} \PYG{n}{starke} \PYG{n}{Knochen}\PYG{o}{.}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

The corresponding keyword in the configuration file is
“health\_claim\_rejected” (see chapter {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{How to configure the program}}}}). This file
can be extended with words to obtain some kind of online learning see
section {\hyperref[\detokenize{online_learning:rejected-online}]{\sphinxcrossref{\DUrole{std,std-ref}{List with rejected health claims:}}}} for more information.


\chapter{Analysis modules}
\label{\detokenize{modules:modules}}\label{\detokenize{modules::doc}}\label{\detokenize{modules:analysis-modules}}
In this chapter each available module is described in more
detail. From this information a user should be able to decide, if a
given module suites their purpose. Additionally the used technologies
and the return values are described, so an interpretation of the
results is easier.


\section{Address extraction (module 1):}
\label{\detokenize{modules:address-extraction-module-1}}
To extract addresses from text, snippets of the text are cut out
around 5-digit words, that resemble german postal codes. The snippets
are then labeled with an conditional random field to extract the
single elements of a possible address, like company name, street name,
house number, postal code, city and country name.

The results are returned in form of a list containing
dictionaries. Each dictionary contains the extracted address elements,
that can be accessed through the corresponding key. The possible keys
are: Unternehmen, Strasse, PLZ, Ort, Land, Bundesland, Kreis. If for
an address one or more elements haven’t been found, the corresponding
value is set to None.


\section{Shop classifier (module 2):}
\label{\detokenize{modules:shop-classifier-module-2}}
The shop classifier consists of a pipeline of different steps (see
chapter {\hyperref[\detokenize{training:training-clf}]{\sphinxcrossref{\DUrole{std,std-ref}{Shop, Food and Product classifier (modules 2, 3, 4):}}}} for more information). The training setup
is the following: First, the given text is tokenized so that only
words with more than two letters are kept and all special characters
are discarded. These tokens are weighted with a common theme (Term
frequency inverse document frequency), stopwords are removed and on
top of this weighted bag-of-words representation a support vector
machine is trained. For predicting the class the same scheme is used.

The result is a probability score which corresponds to the distance to
the hyperplane. The default probability score where it is assumed that
a website is a shop is 50. This threshold is only used for online
learning, so that the user is free to interpret the probability score.


\section{Foodshop classifier (module 3):}
\label{\detokenize{modules:foodshop-classifier-module-3}}
The food classifier consists of a pipeline of different steps (see
chapter {\hyperref[\detokenize{training:training-clf}]{\sphinxcrossref{\DUrole{std,std-ref}{Shop, Food and Product classifier (modules 2, 3, 4):}}}} for more information). The training setup
is the following: First, the given text is tokenized so that only
words with more than two letters are kept and all special characters
are discarded. These tokens are reduced to words in a fixed vocabulary
and weighted with a common theme (Term frequency inverse document
frequency) and on top of this weighted bag-of-words representation a
support vector machine is trained. For predicting the class the same
scheme is used.

The result is a probability score which corresponds to the distance to
the hyperplane. The default probability score where it is assumed that
a website contains food is 40. This threshold is only used for online
learning, so that the user is free to interpret the probability score.


\section{Product page classifier (module 4):}
\label{\detokenize{modules:product-page-classifier-module-4}}
The product classifier consists of a pipeline of different steps (see
chapter {\hyperref[\detokenize{training:training-clf}]{\sphinxcrossref{\DUrole{std,std-ref}{Shop, Food and Product classifier (modules 2, 3, 4):}}}} for more information). The training setup
is the following: First, the given text is tokenized so that only
words with more than two letters are kept and all special characters
are discarded. These tokens are weighted with a common theme (Term
frequency inverse document frequency) and on top of this weighted
bag-of-words representation a support vector machine is trained. For
predicting the class the same scheme is used.

The result is a probability score which corresponds to the distance to
the hyperplane. The default probability score where it is assumed that
a website offers a specific product is 50. This threshold is only used
for online learning, so that the user is free to interpret the
probability score.


\section{Extracting product information (module 5):}
\label{\detokenize{modules:extracting-product-information-module-5}}
This module extracts the product number and the product name. For the
product number a regular expression is used. It is assumed, that
before the product number a commonly used word indicates, that the
following word is the product number. For the product name the title
of the website is extracted and labeled with a conditional random
field. Afterwards all words between the first positive labeled and the
last positive labeled word are returned as product name. Both return
values are strings.


\section{Checking ecological traders (module 6):}
\label{\detokenize{modules:checking-ecological-traders-module-6}}
To identify and validate ecological traders different kind of analysis
are used. First the text is searched with a regular expression if any
word contains “bio”, “öko”, “biologisch”, “ökologisch”. Afterwards a
regular expression matching the specific german “Ökonummer”
(DE-ÖKO-000) is used to extract all german “Ökonummern”. These are
validated against a list with all valid “Ökonummern”. The result for
this analysis is a dictionary with the keys “ads”, “fake” and “legal”,
containing a list each with all ad-words, all not valid “Ökonummern”
and all valid “Ökonummern” found in the text.

Furthermore if a screenshot of the website is available, it is
searched for the official EU logo, that ecological traders must
display. This analysis returns a dictionary with the key “logos” where
the number of logos is stored. This dictionary can be combined with
the dictionary from the text analysis to obtain a dictionary
containing all results relevant for this module.


\section{Checking health claims (module 7):}
\label{\detokenize{modules:checking-health-claims-module-7}}\label{\detokenize{modules:module-hc}}
To detect possible and rejected health claims there are three
strategies available. The first strategy searches only for suspicious
substances and diseases, the second strategy searches for rejected
health claims and the third strategy uses semantic parsing of language
to detect relevant relationships with suspicious substances and
diseases.

The first strategy searches the text for given substances and
diseases. The found words can occur anywhere in the text and don’t
stand always in a relation to each other. Here just the occurence of
these words is enough to arouse suspicion. When at least one disease
is found in the text, a list with the substances and a list with the
diseases is returned. Else only two empty lists are returned.

The second strategy searches for already rejected health claims. These
health claims have to be in the right language and only occurences
with the exact wording and setting are found. If some rejected health
claims are found a list with these health claims is returned,
otherwise only an empty list is returned.

The third strategy uses a semantic parser to get more detailed
relations between suspicious words. Each sentence of the text is
parsed and the verb is identified. If the verb is relevant for this
context, the parsed sentence is reported. To give an additional
ranking of the reported sentences, the occurence of suspicious
substances and diseases is counted and reported along the
sentence. The return value is a list with lists containing the parsed
sentence (a dictionary with the different phrases) and a ranking
value. If no relevant verbs were found, an empty list is returned.


\section{Checking PDO, PGI and TSG (module 8):}
\label{\detokenize{modules:checking-pdo-pgi-and-tsg-module-8}}
To identify products that are registered in the EU door list and
therefor have a certificate (PDO, PGI or TSG) the product name has to
be extracted from the website (compare module 5). In order to check a
given product name against the entries in the door list a
normalization scheme has to be applied to both sides. One complication
in this matter is that the door list contains only product names in
the original language. This can lead to worse results for products in
other languages than german, because only german words are
normalized. All other product names are just splitted on whitespace
characters and converted to lower case. For the normalization step
all words are stemmed and stop words are removed.

To search a product name in the preprocessed door list, the product
name is normalized with the german scheme and the scheme for foreign
languages. After that all possible n-grams of the product name are
searched for in the preprocessed door list and the corresponding
cerfiticate group (PDO, PGI or TSG) is returned when the product name
was found. If the product name couldn’t be found in the preprocessed
door list None is returned.


\section{Extracting ingredients (module 9):}
\label{\detokenize{modules:extracting-ingredients-module-9}}
This module extracts the list of ingredients from a website. To do so
three different pretrained statistical models and a conditional random
field are used. At first, a regular expression is used to extract at
least 150 words after the word “Zutaten”, which has to be in front of
a list of ingredients after EU law. Each extracted word is assigned
the probability, with which it is in a list of ingredients, the
probability, with which it is in a normal text, and the probability,
with which it is at the end of a list of ingredients. For the last
probability the context is considered too and the maximum probability
is taken. On the three assigned probabilities a conditional random
field is used to determine the end of the list of ingredients. The
return value consists of a list of results for every occurence of the
word “Zutaten” in the text. For each word a dictionary with all words
from the determined list of ingredients (key: “ingredients”) and the
count of occurences of the last word as indicator for the likelihood
of this exact end (key: “count”) is added to the list.


\section{Checking BioC (module 10):}
\label{\detokenize{modules:checking-bioc-module-10}}
This module validates the EU certificate for ecological traders
against one version of the BioC database. This version is from
February 2018. To check if a trader has a certificate, this modules
takes a normalized address and looks this address up in a pre-build
dictionary. If there exists at least one certificate for this address,
the information from this certificate is returned in a dictionary. The
dictionary contains the following keys and values: “numbers”: all
“Ökonummern” stored within the BioC for the found certificate;
“periods”: the periods in which the certificate is valid (given by
day, month and year in a dictionary); and “address”: the address that
was used in the certificate.


\chapter{Overall structure}
\label{\detokenize{overall:overall-structure}}\label{\detokenize{overall::doc}}
\noindent{\hspace*{\fill}\sphinxincludegraphics[scale=0.28]{{structure}.png}\hspace*{\fill}}


\chapter{How to use online-learning with the program}
\label{\detokenize{online_learning:how-to-use-online-learning-with-the-program}}\label{\detokenize{online_learning::doc}}\label{\detokenize{online_learning:online}}
Some classifiers and external data files can be updated on a regular
basis to obtain some kind of oneline-learning for the backend. The
modules 2, 3, 4, 7 and 9 can benefit from updating. In which way each
module can be improved by updating the classifiers or external data is
described in the following chapter. Other modules, that rely on
external data, such as modules 1, 6 and 8 can’t be updated to improve
the results. Nevertheless the external data for these modules should
be kept up to date to prevent false results.


\section{The shop, food and product classifier (modules 2, 3, 4):}
\label{\detokenize{online_learning:the-shop-food-and-product-classifier-modules-2-3-4}}
It is assumed, that in your workflow, you reevaluate at least some
results from the classifiers. This manual classification results
should be transfered back to the database table containing the
results. In the database table “results” the column “manual\_analysis”
is for this purpose. When a manual classification result of one of the
modules 2, 3 or 4 is transfered back, the flag “validation\_result”
should be set to 1. After using this information for online-learning
the flag “updated\_results” is set to 1.

For a given interval (see chapter {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{How to configure the program}}}}) the program
extracts all results, for which the validation flag is 1. For all
these results the value of the manual analysis is compared to the
value of the automatic analysis for each of these modules and only
when they differ the corresponding module is refined with
online-learning on this data point.


\section{Adding words to the food vocabulary (module 3):}
\label{\detokenize{online_learning:food-vocab-label}}\label{\detokenize{online_learning:adding-words-to-the-food-vocabulary-module-3}}
The food classifier (module 3) uses a fixed vocabulary to classify
text. You can add more words to this vocabulary by extending the file
“food\_vocab” (see chapter {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{How to configure the program}}}}). The file contains in
every line one word, like:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Apfel}
\PYG{n}{Banane}
\PYG{n}{Citrone}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

To ensure, that your words can succesfully be used, follow this
format. The words are transformed to lower characters and the file is
assumed to be in utf-8 encoding.


\section{Adding information to the white- and blacklist for ingredients (module 9):}
\label{\detokenize{online_learning:black-white-online}}\label{\detokenize{online_learning:adding-information-to-the-white-and-blacklist-for-ingredients-module-9}}
Even though the white and blacklist aren’t directly used to build the
statistical word model for ingredints list, they are used to find
possibly not allowed ingredients and prohibited ingredients. To add
new words to this list, you can extend the files
“ingredients\_whitelist” and “ingredients\_blacklist” (see chapter
{\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{How to configure the program}}}}).

Both files have the following format:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Milch}
\PYG{n}{Zitronen}
\PYG{n}{flüssig}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

So each line contains one word. If there are some words, that are only
meaningful together, you can add both words in one line. But note,
that in this case only the two words seperated with the exact same
character (e.g. space) are searched for. The words are transformed to
lower characters and the file is assumed to be in utf-8 encoding.


\section{Adding information to several lists to check health claims (module 7):}
\label{\detokenize{online_learning:adding-information-to-several-lists-to-check-health-claims-module-7}}
To detect possible health claims, several different strategies can be
used (see section {\hyperref[\detokenize{modules:module-hc}]{\sphinxcrossref{\DUrole{std,std-ref}{Checking health claims (module 7):}}}}). For each strategie several
external information is used and can be extended. There exist four
files containing the different informations: one file with possibly
used substances in health claims, one file with possibly used diseases
in health claims, one file with rejected health claims and one file
with relevant verbs. The files are described in chapter
{\hyperref[\detokenize{external_data:external-data}]{\sphinxcrossref{\DUrole{std,std-ref}{External data}}}} in more detail.


\subsection{List with substances:}
\label{\detokenize{online_learning:list-with-substances}}\label{\detokenize{online_learning:substances-online}}
This file should contain substances, that are often used in health
claims. The file is in the following format:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Vitamin} \PYG{n}{C}
\PYG{n}{Eisen}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

The list contains also phrases with more than one word and you can
extend this list with these too. Note that only occurences with the
excat same delimiting character (e.g. space) are searched for. The
words are transformed to lower characters and the file is assumed to
be in utf-8 encoding.


\subsection{List with disease:}
\label{\detokenize{online_learning:diseases-online}}\label{\detokenize{online_learning:list-with-disease}}
This file should contain disease, that are often used in health
claims. The file is in the following format:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Herzinfarkt}
\PYG{n}{rote} \PYG{n}{Blutkörperchen}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

The list contains also phrases with more than one word and you can
extend this list with these too. Note that only occurences with the
excat same delimiting character (e.g. space) are searched for. The
words are transformed to lower characters and the file is assumed to
be in utf-8 encoding.


\subsection{List with verb declinations:}
\label{\detokenize{online_learning:declination-online}}\label{\detokenize{online_learning:list-with-verb-declinations}}
This file contains different verbs with relevant declinations. Based
on this list, sentences with not relevant verbforms are filtered out
and not considered a possible health claim. The file is in the
following format:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{beitragen}
\PYG{n}{trug} \PYG{n}{bei}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

The list contains also phrases with more than one word and you can
extend this list with these too. Note that only occurences with the
excat same delimiting character (e.g. space) are searched for. The
words are transformed to lower characters and the file is assumed to
be in utf-8 encoding.


\subsection{List with rejected health claims:}
\label{\detokenize{online_learning:rejected-online}}\label{\detokenize{online_learning:list-with-rejected-health-claims}}
This file should contain rejected health claims translated to
german. The file is in the following format:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Actimirell} \PYG{n}{aktiviert} \PYG{n}{Abwehkräfte}\PYG{o}{.}
\PYG{n}{Milchschneideling} \PYG{n}{macht} \PYG{n}{starke} \PYG{n}{Knochen}\PYG{o}{.}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

The list contains only phrases with multiple words. Only a simple
search is performed, where the exact wording (with delimiters and
punctuation) is found. But the words are transformed to lower characters
and the file is assumed to be in utf-8 encoding.


\chapter{How to train and test the single classifiers}
\label{\detokenize{training:how-to-train-and-test-the-single-classifiers}}\label{\detokenize{training:training}}\label{\detokenize{training::doc}}
This chapter explains the necessary files and formats to train or test
the single classifiers. The programs offers a commandline interface
for this task, which is explained in chapter {\hyperref[\detokenize{commandline:commandline}]{\sphinxcrossref{\DUrole{std,std-ref}{Commandline Interface}}}}. Some
modules presented in chapter {\hyperref[\detokenize{modules:modules}]{\sphinxcrossref{\DUrole{std,std-ref}{Analysis modules}}}} use classifiers to analyse
and extract data. These modules are the shop (module 2), food
(module 3) and product (module 4) classifier, the address extraction
(module 1) and the product name extraction (module 5) modules.

If you want to measure the performance of more than one module you
should add single jobs to the database and let the program run
normally (see chapter XX and XX).


\section{Shop, Food and Product classifier (modules 2, 3, 4):}
\label{\detokenize{training:shop-food-and-product-classifier-modules-2-3-4}}\label{\detokenize{training:training-clf}}
The classifier for the shop, food and product domain consists of a
support vector machine with a bag-of-words and a random forest as
feature selection. For each domain some parameters were chosen by
cross-validation on a specific training data set and are hard-coded in
the program. So they can not easily be changed or evaluated. The
parameters relate to the feature extraction, the feature selection and
a hyperparameter from the support vector machine.

The chosen parameters for each domain are:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxstylestrong{pipeline step}
&\sphinxstyletheadfamily 
\sphinxstylestrong{parameter}
&\sphinxstyletheadfamily 
\sphinxstylestrong{shop}
&\sphinxstyletheadfamily 
\sphinxstylestrong{food}
&\sphinxstyletheadfamily 
\sphinxstylestrong{product}
\\
\hline\sphinxmultirow{2}{6}{%
\begin{varwidth}[t]{\sphinxcolwidth{1}{5}}
tfidf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
}%
&
stop-word removal
&
yes
&
no
&
no
\\
\cline{2-5}\sphinxtablestrut{6}&
fixed vocabulary
&
no
&
yes
&
no
\\
\hline\sphinxmultirow{2}{15}{%
\begin{varwidth}[t]{\sphinxcolwidth{1}{5}}
random forest
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
}%
&
nof estimators
&
100
&
0
&
100
\\
\cline{2-5}\sphinxtablestrut{15}&
threshold
&
0.0004
&
0
&
0.0004
\\
\hline
svm
&
alpha
&
0.00001
&
0.0001
&
0.0000001
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

To train a classifier for one of the three domains, a training set is
needed. It should consist of either texts from webpages in utf-8
encoding or directly of stored webpages. For each webpage in the
training set the corresponding class should be manually assigned
(e.g. shop or no shop) for better performance of the trained
classifier.

A special directory structure is assumed by the training and testing
methods of the classifier. A directory with two subdirectories
containing files for each class should be provided, like so:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{train}\PYG{o}{/}
  \PYG{l+m+mi}{0}\PYG{o}{/}
    \PYG{n}{file1}\PYG{o}{.}\PYG{n}{html}
    \PYG{n}{file2}\PYG{o}{.}\PYG{n}{html}
    \PYG{n}{file3}\PYG{o}{.}\PYG{n}{html}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
  \PYG{l+m+mi}{1}\PYG{o}{/}
    \PYG{n}{file4}\PYG{o}{.}\PYG{n}{html}
    \PYG{n}{file5}\PYG{o}{.}\PYG{n}{html}
    \PYG{n}{file6}\PYG{o}{.}\PYG{n}{html}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

To obtain the right measurements while testing, all positive examples
(like shop, food or product) should be within the directory 0/ while
all negative examples (like no shop, no food or no product) should be
within the directory 1/. The calculation of precision and recall is
based on the assumption, that the classes are assigned this way.

In general the performance of the classifier can be measured with a
test set in the same directory structure as the training set. The test
set should not be used for training.


\section{Address extraction (module 1):}
\label{\detokenize{training:address-extraction-module-1}}
The extraction of addresses is learned with a conditional random field
(CRF). To train a CRF a set of labeled sequences is needed. Each
sequence contains text seperated by whitespace (except newline)
characters and to each token a label has to be assigned. When a token
is irrelevant the label OT (other) can be assigned. The module
extracts the tokens with the following labels: FN (company name), ST
(street), NR (house number), PLZ (postal code), CI (city) and CO
(country).

To train a CRF one file with the text (x-file) and one file with the
labels (y-file) has to be provided. The files can look somewhat like
this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
x\PYGZhy{}file.txt
  Hier sitzt die Firma: Musterman AG A\PYGZhy{}Straße 123 98765 Krautheim komm vorbei
  Trifft dich Hansi Hinterseer B\PYGZhy{}Straße 456 12345 Alpenort Deutschland schönes Date!

y\PYGZhy{}file.txt
  OT OT OT OT FN FN ST NR PLZ CI OT OT
  OT OT OT OT ST NR PLZ CI CO OT OT
\end{sphinxVerbatim}

Note: The lines of text correspond with the lines of labels and not
every label has to be present in every sequence (e.g. the first line
contains no CO label and in the second example “Hansi Hinterseer” is a
name of a person and not a company, so not labeled FN). But you should
only add examples you want the CRF to recognize to your training data
set. So leave out all the address fragments, when you don’t want to
extract them too.

To test the performance of your CRF you can provide an unseen x-file
and corresponding y-file to the classifier. There is functionality in
the program to measure the performance of the trained CRF for every
label.


\section{Product name extraction (module 5):}
\label{\detokenize{training:product-name-extraction-module-5}}
Like the address extraction, the product name extraction uses a CRF to
label a token sequence. The label set consists of two different
labels: OT (other) and AN (article name). The input to the pretrained
CRF were titles of webpages and their manually assigned labels. The
titles were tokenized, so that only words of minimum length 2 and
special characters are extracted. You should tokenize your input for
training or testing in a similar way.

An example input looks like this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{o}{\PYGZhy{}}\PYG{n}{file}\PYG{o}{.}\PYG{n}{txt}
  \PYG{n}{Amore} \PYG{n}{kaufen} \PYG{p}{:} \PYG{n}{Bratwurst} \PYG{n}{Henning} \PYG{l+m+mi}{180} \PYG{n}{gr} \PYG{o}{\PYGZhy{}} \PYG{n}{nur} \PYG{n}{hier}
  \PYG{n}{laden} \PYG{n}{online} \PYG{o}{\PYGZhy{}} \PYG{n}{salzige} \PYG{n}{gurken} \PYG{p}{,} \PYG{n}{lose}
\PYG{n}{y}\PYG{o}{\PYGZhy{}}\PYG{n}{file}\PYG{o}{.}\PYG{n}{txt}
  \PYG{n}{OT} \PYG{n}{OT} \PYG{n}{OT} \PYG{n}{AN} \PYG{n}{AN} \PYG{n}{AN} \PYG{n}{AN} \PYG{n}{OT} \PYG{n}{OT} \PYG{n}{OT}
  \PYG{n}{OT} \PYG{n}{OT} \PYG{n}{OT} \PYG{n}{AN} \PYG{n}{AN} \PYG{n}{AN} \PYG{n}{AN}
\end{sphinxVerbatim}

To test the performance of your CRF you can provide an unseen x-file
and corresponding y-file to the classifier. There is functionality in
the program to measure the performance of the trained CRF for every
label.


\chapter{Commandline Interface}
\label{\detokenize{commandline:commandline}}\label{\detokenize{commandline::doc}}\label{\detokenize{commandline:commandline-interface}}
The commandline interface for the program allows the user to perform
multiple tasks. The single tasks can be chosen by subcommands. In the
following chapter, the description for each subcommand is shown. The
same output is available via the \textendash{}help flag.


\section{General interface:}
\label{\detokenize{commandline:general-interface}}
The general interface offers subcommands for training, testing,
updating more testing and the usage of the program as tool. It
contains the following subcommands and optional arguments:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{config} \PYG{n}{CONFIG}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{debug}\PYG{p}{]}
                  \PYG{p}{\PYGZob{}}\PYG{n}{train}\PYG{p}{,}\PYG{n}{test}\PYG{p}{,}\PYG{n}{update}\PYG{p}{,}\PYG{n}{load}\PYG{p}{,}\PYG{n}{run}\PYG{p}{\PYGZcb{}} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}

\PYG{n}{Backend} \PYG{k}{for} \PYG{n}{the} \PYG{n}{AAPVL}\PYG{o}{\PYGZhy{}}\PYG{n}{Project}\PYG{o}{.}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
 \PYG{p}{\PYGZob{}}\PYG{n}{train}\PYG{p}{,}\PYG{n}{test}\PYG{p}{,}\PYG{n}{update}\PYG{p}{,}\PYG{n}{load}\PYG{p}{,}\PYG{n}{run}\PYG{p}{\PYGZcb{}}
   \PYG{n}{train}               \PYG{n}{train} \PYG{n}{different} \PYG{n}{classifier}
   \PYG{n}{test}                \PYG{n}{test} \PYG{n}{different} \PYG{n}{classifier} \PYG{n}{directly} \PYG{o+ow}{and} \PYG{n}{test} \PYG{n}{different} \PYG{n}{functionalities}
   \PYG{n}{update}              \PYG{n}{update} \PYG{n}{the} \PYG{n}{different} \PYG{n}{classifier} \PYG{k}{with} \PYG{n}{new} \PYG{n}{data} \PYG{k+kn}{from} \PYG{n+nn}{a} \PYG{n}{directory}
   \PYG{n}{load}                \PYG{n}{load} \PYG{n}{data} \PYG{k+kn}{from} \PYG{n+nn}{a} \PYG{n}{directory} \PYG{n}{into} \PYG{n}{database}\PYG{o}{.} \PYG{n}{this} \PYG{n}{can} \PYG{n}{be} \PYG{n}{used} \PYG{k}{for} \PYG{n}{testing}
   \PYG{n}{run}                 \PYG{n}{run} \PYG{n}{the} \PYG{n}{backend} \PYG{o+ow}{and} \PYG{n}{process} \PYG{n}{jobs}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
 \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}            \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
 \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{config} \PYG{n}{CONFIG}       \PYG{n}{line} \PYG{n}{based} \PYG{n}{configuration} \PYG{n}{file}
 \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{debug}               \PYG{n}{enable} \PYG{n}{debug}\PYG{o}{\PYGZhy{}}\PYG{n}{information} \PYG{o+ow}{in} \PYG{n}{log}\PYG{o}{\PYGZhy{}}\PYG{n}{file}
\end{sphinxVerbatim}

The help messages for the subcommands load and run are shown here in
detail, while the help messages for the other subcommands are shown in
the following sections.

Commandline interface for subcommand load:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{load} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{modules} \PYG{n}{MODULES}\PYG{p}{]} \PYG{n+nb}{dir}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n+nb}{dir}                \PYG{n}{path} \PYG{n}{to} \PYG{n}{directory} \PYG{k+kn}{from} \PYG{n+nn}{which} \PYG{n+nb}{all} \PYG{n}{files} \PYG{n}{are} \PYG{n}{added} \PYG{n}{to} \PYG{n}{the} \PYG{n}{database}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}         \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
  \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{modules} \PYG{n}{MODULES}  \PYG{n}{comma} \PYG{n}{separated} \PYG{n+nb}{list} \PYG{n}{of} \PYG{n}{modules} \PYG{n}{that} \PYG{n}{are} \PYG{n}{added} \PYG{n}{to} \PYG{n}{the} \PYG{n}{database} \PYG{k}{for} \PYG{n}{every} \PYG{n}{file}\PYG{o}{.} \PYG{k}{if} \PYG{n}{omitted}\PYG{p}{,} \PYG{n+nb}{all} \PYG{n}{modules} \PYG{n}{are} \PYG{n}{registered}
\end{sphinxVerbatim}

Commandline interface for subcommand run:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{run} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}


\section{Interface for training:}
\label{\detokenize{commandline:interface-for-training}}
Commandline interface for subcommand train:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{train} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{p}{\PYGZob{}}\PYG{n}{shop}\PYG{p}{,}\PYG{n}{food}\PYG{p}{,}\PYG{n}{product}\PYG{p}{,}\PYG{n}{imp}\PYG{p}{,}\PYG{n}{prod}\PYG{o}{\PYGZhy{}}\PYG{n}{name}\PYG{p}{\PYGZcb{}} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{p}{\PYGZob{}}\PYG{n}{shop}\PYG{p}{,}\PYG{n}{food}\PYG{p}{,}\PYG{n}{product}\PYG{p}{,}\PYG{n}{imp}\PYG{p}{,}\PYG{n}{prod}\PYG{o}{\PYGZhy{}}\PYG{n}{name}\PYG{p}{\PYGZcb{}}
    \PYG{n}{shop}                \PYG{n}{train} \PYG{n}{the} \PYG{n}{shop} \PYG{n}{classifier} \PYG{k}{with} \PYG{n}{the} \PYG{n}{data} \PYG{k+kn}{from} \PYG{n+nn}{DIR}
    \PYG{n}{food}                \PYG{n}{train} \PYG{n}{the} \PYG{n}{shop} \PYG{n}{classifier} \PYG{k}{with} \PYG{n}{the} \PYG{n}{data} \PYG{k+kn}{from} \PYG{n+nn}{DIR}
    \PYG{n}{product}             \PYG{n}{train} \PYG{n}{the} \PYG{n}{product} \PYG{n}{classifier} \PYG{k}{with} \PYG{n}{the} \PYG{n}{data} \PYG{k+kn}{from} \PYG{n+nn}{DIR}
    \PYG{n}{imp}                 \PYG{n}{train} \PYG{n}{the} \PYG{n}{crf} \PYG{k}{for} \PYG{n}{address} \PYG{n}{extraction}
    \PYG{n}{prod}\PYG{o}{\PYGZhy{}}\PYG{n}{name}           \PYG{n}{train} \PYG{n}{the} \PYG{n}{crf} \PYG{k}{for} \PYG{n}{product} \PYG{n}{name} \PYG{n}{extraction}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}            \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}

Commandline interface for subcommand train shop:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{train} \PYG{n}{shop} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{n+nb}{dir}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n+nb}{dir}         \PYG{n}{directory} \PYG{k}{with} \PYG{n}{data}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}

Commandline interface for subcommand train food:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{train} \PYG{n}{food} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{n+nb}{dir}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n+nb}{dir}         \PYG{n}{directory} \PYG{k}{with} \PYG{n}{data}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}

Commandline interface for subcommand train product:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{train} \PYG{n}{product} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{n+nb}{dir}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n+nb}{dir}         \PYG{n}{directory} \PYG{k}{with} \PYG{n}{data}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}

Commandline interface for subcommand train imp:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{train} \PYG{n}{imp} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{n}{x} \PYG{n}{y}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n}{x}           \PYG{n}{file} \PYG{k}{with} \PYG{n}{addresses} \PYG{o+ow}{or} \PYG{n}{titles} \PYG{n}{of} \PYG{n}{websites} \PYG{n}{respectively} \PYG{o+ow}{in} \PYG{n}{each} \PYG{n}{line}
  \PYG{n}{y}           \PYG{n}{file} \PYG{k}{with} \PYG{n}{label} \PYG{n}{sequences} \PYG{o+ow}{in} \PYG{n}{each} \PYG{n}{line} \PYG{n}{corresponding} \PYG{n}{to} \PYG{n}{the} \PYG{n}{tokens} \PYG{o+ow}{in} \PYG{n}{X}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}

Commandline interface for subcommand train prod-name:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{train} \PYG{n}{prod}\PYG{o}{\PYGZhy{}}\PYG{n}{name} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{n}{x} \PYG{n}{y}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n}{x}           \PYG{n}{file} \PYG{k}{with} \PYG{n}{addresses} \PYG{o+ow}{or} \PYG{n}{titles} \PYG{n}{of} \PYG{n}{websites} \PYG{n}{respectively} \PYG{o+ow}{in} \PYG{n}{each} \PYG{n}{line}
  \PYG{n}{y}           \PYG{n}{file} \PYG{k}{with} \PYG{n}{label} \PYG{n}{sequences} \PYG{o+ow}{in} \PYG{n}{each} \PYG{n}{line} \PYG{n}{corresponding} \PYG{n}{to} \PYG{n}{the} \PYG{n}{tokens} \PYG{o+ow}{in} \PYG{n}{X}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}


\section{Interface for testing:}
\label{\detokenize{commandline:interface-for-testing}}
Commandline interface for subcommand test:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{test} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{p}{\PYGZob{}}\PYG{n}{shop}\PYG{p}{,}\PYG{n}{food}\PYG{p}{,}\PYG{n}{product}\PYG{p}{,}\PYG{n}{imp}\PYG{p}{,}\PYG{n}{prod}\PYG{o}{\PYGZhy{}}\PYG{n}{name}\PYG{p}{\PYGZcb{}} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{p}{\PYGZob{}}\PYG{n}{shop}\PYG{p}{,}\PYG{n}{food}\PYG{p}{,}\PYG{n}{product}\PYG{p}{,}\PYG{n}{imp}\PYG{p}{,}\PYG{n}{prod}\PYG{o}{\PYGZhy{}}\PYG{n}{name}\PYG{p}{\PYGZcb{}}
    \PYG{n}{shop}                \PYG{n}{test} \PYG{n}{the} \PYG{n}{shop} \PYG{n}{classifier} \PYG{k}{with} \PYG{n}{the} \PYG{n}{data} \PYG{k+kn}{from} \PYG{n+nn}{DIR}
    \PYG{n}{food}                \PYG{n}{test} \PYG{n}{the} \PYG{n}{shop} \PYG{n}{classifier} \PYG{k}{with} \PYG{n}{the} \PYG{n}{data} \PYG{k+kn}{from} \PYG{n+nn}{DIR}
    \PYG{n}{product}             \PYG{n}{test} \PYG{n}{the} \PYG{n}{product} \PYG{n}{classifier} \PYG{k}{with} \PYG{n}{the} \PYG{n}{data} \PYG{k+kn}{from} \PYG{n+nn}{DIR}
    \PYG{n}{imp}                 \PYG{n}{test} \PYG{n}{the} \PYG{n}{crf} \PYG{k}{for} \PYG{n}{address} \PYG{n}{extraction}
    \PYG{n}{prod}\PYG{o}{\PYGZhy{}}\PYG{n}{name}           \PYG{n}{test} \PYG{n}{the} \PYG{n}{crf} \PYG{k}{for} \PYG{n}{product} \PYG{n}{name} \PYG{n}{extraction}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}            \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}

Commandline interface for subcommand test shop:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{test} \PYG{n}{shop} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{n+nb}{dir}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n+nb}{dir}         \PYG{n}{directory} \PYG{k}{with} \PYG{n}{data}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}

Commandline interface for subcommand test food:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{test} \PYG{n}{food} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{n+nb}{dir}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n+nb}{dir}         \PYG{n}{directory} \PYG{k}{with} \PYG{n}{data}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}

Commandline interface for subcommand test product:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{test} \PYG{n}{product} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{n+nb}{dir}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n+nb}{dir}         \PYG{n}{directory} \PYG{k}{with} \PYG{n}{data}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}

Commandline interface for subcommand test imp:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{test} \PYG{n}{imp} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{n}{x} \PYG{n}{y}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n}{x}           \PYG{n}{file} \PYG{k}{with} \PYG{n}{addresses} \PYG{o+ow}{or} \PYG{n}{titles} \PYG{n}{of} \PYG{n}{websites} \PYG{n}{respectively} \PYG{o+ow}{in} \PYG{n}{each} \PYG{n}{line}
  \PYG{n}{y}           \PYG{n}{file} \PYG{k}{with} \PYG{n}{label} \PYG{n}{sequences} \PYG{o+ow}{in} \PYG{n}{each} \PYG{n}{line} \PYG{n}{corresponding} \PYG{n}{to} \PYG{n}{the} \PYG{n}{tokens} \PYG{o+ow}{in} \PYG{n}{X}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}

Commandline interface for subcommand test prod-name:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{test} \PYG{n}{prod}\PYG{o}{\PYGZhy{}}\PYG{n}{name} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{n}{x} \PYG{n}{y}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n}{x}           \PYG{n}{file} \PYG{k}{with} \PYG{n}{addresses} \PYG{o+ow}{or} \PYG{n}{titles} \PYG{n}{of} \PYG{n}{websites} \PYG{n}{respectively} \PYG{o+ow}{in} \PYG{n}{each} \PYG{n}{line}
  \PYG{n}{y}           \PYG{n}{file} \PYG{k}{with} \PYG{n}{label} \PYG{n}{sequences} \PYG{o+ow}{in} \PYG{n}{each} \PYG{n}{line} \PYG{n}{corresponding} \PYG{n}{to} \PYG{n}{the} \PYG{n}{tokens} \PYG{o+ow}{in} \PYG{n}{X}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}


\section{Interface for updating:}
\label{\detokenize{commandline:interface-for-updating}}
Commandline interface for subcommand update:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{update} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{p}{\PYGZob{}}\PYG{n}{shop}\PYG{p}{,}\PYG{n}{food}\PYG{p}{,}\PYG{n}{product}\PYG{p}{,}\PYG{n}{imp}\PYG{p}{,}\PYG{n}{prod}\PYG{o}{\PYGZhy{}}\PYG{n}{name}\PYG{p}{\PYGZcb{}} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{p}{\PYGZob{}}\PYG{n}{shop}\PYG{p}{,}\PYG{n}{food}\PYG{p}{,}\PYG{n}{product}\PYG{p}{,}\PYG{n}{imp}\PYG{p}{,}\PYG{n}{prod}\PYG{o}{\PYGZhy{}}\PYG{n}{name}\PYG{p}{\PYGZcb{}}
    \PYG{n}{shop}                \PYG{n}{update} \PYG{n}{the} \PYG{n}{shop} \PYG{n}{classifier} \PYG{k}{with} \PYG{n}{the} \PYG{n}{data} \PYG{k+kn}{from} \PYG{n+nn}{DIR}
    \PYG{n}{food}                \PYG{n}{update} \PYG{n}{the} \PYG{n}{shop} \PYG{n}{classifier} \PYG{k}{with} \PYG{n}{the} \PYG{n}{data} \PYG{k+kn}{from} \PYG{n+nn}{DIR}
    \PYG{n}{product}             \PYG{n}{update} \PYG{n}{the} \PYG{n}{product} \PYG{n}{classifier} \PYG{k}{with} \PYG{n}{the} \PYG{n}{data} \PYG{k+kn}{from} \PYG{n+nn}{DIR}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}            \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}

Commandline interface for subcommand update shop:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{update} \PYG{n}{shop} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{n+nb}{dir}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n+nb}{dir}         \PYG{n}{directory} \PYG{k}{with} \PYG{n}{data}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}

Commandline interface for subcommand update food:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{update} \PYG{n}{food} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{n+nb}{dir}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n+nb}{dir}         \PYG{n}{directory} \PYG{k}{with} \PYG{n}{data}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}

Commandline interface for subcommand update product:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{usage}\PYG{p}{:} \PYG{n}{backend}\PYG{o}{.}\PYG{n}{py} \PYG{n}{update} \PYG{n}{product} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{n+nb}{dir}

\PYG{n}{positional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{n+nb}{dir}         \PYG{n}{directory} \PYG{k}{with} \PYG{n}{data}

\PYG{n}{optional} \PYG{n}{arguments}\PYG{p}{:}
  \PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{help}  \PYG{n}{show} \PYG{n}{this} \PYG{n}{help} \PYG{n}{message} \PYG{o+ow}{and} \PYG{n}{exit}
\end{sphinxVerbatim}


\chapter{API reference for the AAPVL backend}
\label{\detokenize{api::doc}}\label{\detokenize{api:api-reference-for-the-aapvl-backend}}

\section{The main entry point: Backend}
\label{\detokenize{api:the-main-entry-point-backend}}\label{\detokenize{api:module-backend}}\index{backend (module)}\index{create\_parser() (in module backend)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:backend.create_parser}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{backend.}}\sphinxbfcode{\sphinxupquote{create\_parser}}}{}{}
Create the argument parser.

\end{fulllineitems}

\index{load\_db() (in module backend)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:backend.load_db}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{backend.}}\sphinxbfcode{\sphinxupquote{load\_db}}}{\emph{config}, \emph{directory}, \emph{modules}}{}
Load data in database for broder test.

For every file in the directory a job is submitted to the
database. The file is registered for every module in
modules. Other options aren’t available. Each file is treated, as
if it was its own main page.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{config} \textendash{} directory with important configuration information

\item {} 
\sphinxstylestrong{directory} \textendash{} path to directory with test data

\item {} 
\sphinxstylestrong{modules} \textendash{} comma separated string with module numbers

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{main() (in module backend)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:backend.main}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{backend.}}\sphinxbfcode{\sphinxupquote{main}}}{}{}
Main entry point.

\end{fulllineitems}

\index{read\_config() (in module backend)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:backend.read_config}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{backend.}}\sphinxbfcode{\sphinxupquote{read\_config}}}{\emph{filename}}{}
Parse the config file.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{filename} \textendash{} filename of the config file

\end{description}\end{quote}

\end{fulllineitems}

\index{test\_clf() (in module backend)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:backend.test_clf}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{backend.}}\sphinxbfcode{\sphinxupquote{test\_clf}}}{\emph{config}, \emph{directory}, \emph{type\_}}{}
Test the different classifier.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{directory} \textendash{} path to directory with test data

\item {} 
\sphinxstylestrong{type\_} \textendash{} type of the classifier. one of: ‘shop’, ‘food’ or ‘product’

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{test\_impressum() (in module backend)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:backend.test_impressum}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{backend.}}\sphinxbfcode{\sphinxupquote{test\_impressum}}}{\emph{x\_file}, \emph{y\_file}}{}
Test the impressum crf.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{x\_file} \textendash{} file with test sequences

\item {} 
\sphinxstylestrong{y\_file} \textendash{} file with test labels for the sequences

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{test\_product\_name() (in module backend)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:backend.test_product_name}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{backend.}}\sphinxbfcode{\sphinxupquote{test\_product\_name}}}{\emph{x\_file}, \emph{y\_file}}{}
Test the product name crf.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{x\_file} \textendash{} file with test sequences

\item {} 
\sphinxstylestrong{y\_file} \textendash{} file with test labels for the sequences

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{test\_simple() (in module backend)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:backend.test_simple}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{backend.}}\sphinxbfcode{\sphinxupquote{test\_simple}}}{\emph{config}, \emph{max\_tries}, \emph{delay}, \emph{delay\_module}, \emph{day}, \emph{update\_rate}}{}
Test the general setup of the backend.

Loads some test jobs into the database and runs every module on
them to check the general setup.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{config} \textendash{} dictionary with important configuration information

\item {} 
\sphinxstylestrong{max\_tries} \textendash{} maximal number of tries for worker

\item {} 
\sphinxstylestrong{delay} \textendash{} delay in seconds after no job was found

\item {} 
\sphinxstylestrong{delay\_module} \textendash{} time in seconds a module is allowed to take

\item {} 
\sphinxstylestrong{day} \textendash{} number of times to check the database for jobs per day

\item {} 
\sphinxstylestrong{update\_rate} \textendash{} number of days after which online learning should
be performed

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{train\_clf() (in module backend)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:backend.train_clf}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{backend.}}\sphinxbfcode{\sphinxupquote{train\_clf}}}{\emph{config}, \emph{directory}, \emph{type\_}}{}
Train the different classifiers.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{directory} \textendash{} path to directory with training data

\item {} 
\sphinxstylestrong{type\_} \textendash{} type of the classifier. one of: ‘shop’, ‘food’ or ‘product’

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{train\_impressum() (in module backend)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:backend.train_impressum}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{backend.}}\sphinxbfcode{\sphinxupquote{train\_impressum}}}{\emph{x\_file}, \emph{y\_file}}{}
Train the impressum crf.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{x\_file} \textendash{} file with training sequences

\item {} 
\sphinxstylestrong{y\_file} \textendash{} file with training labels for the sequences

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{train\_product\_name() (in module backend)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:backend.train_product_name}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{backend.}}\sphinxbfcode{\sphinxupquote{train\_product\_name}}}{\emph{x\_file}, \emph{y\_file}}{}
Train the product name crf.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{x\_file} \textendash{} file with training sequences

\item {} 
\sphinxstylestrong{y\_file} \textendash{} file with training labels for the sequences

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{update\_clf() (in module backend)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:backend.update_clf}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{backend.}}\sphinxbfcode{\sphinxupquote{update\_clf}}}{\emph{config}, \emph{directory}, \emph{type\_}}{}
Update the different classifiers with online learning.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{directory} \textendash{} path to directory with new training data

\item {} 
\sphinxstylestrong{type\_} \textendash{} type of the classifier. one of: ‘shop’, ‘food’ or ‘product’

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}



\section{This is where the magic happens: Worker}
\label{\detokenize{api:this-is-where-the-magic-happens-worker}}\label{\detokenize{api:module-worker}}\index{worker (module)}\index{Worker (class in worker)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{worker.}}\sphinxbfcode{\sphinxupquote{Worker}}}{\emph{config}, \emph{delay\_module=180}}{}
Worker combines all the modules to get the jobs and process them.
\index{\_delay\_module (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker._delay_module}}\pysigline{\sphinxbfcode{\sphinxupquote{\_delay\_module}}}
time after that a module should be stopped

\end{fulllineitems}

\index{\_interface (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker._interface}}\pysigline{\sphinxbfcode{\sphinxupquote{\_interface}}}
interface to the database

\end{fulllineitems}

\index{\_lm\_theta (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker._lm_theta}}\pysigline{\sphinxbfcode{\sphinxupquote{\_lm\_theta}}}
probability threshold for the lm-classifier

\end{fulllineitems}

\index{\_shop\_theta (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker._shop_theta}}\pysigline{\sphinxbfcode{\sphinxupquote{\_shop\_theta}}}
probability threshold for the shop-classifier

\end{fulllineitems}

\index{\_product\_theta (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker._product_theta}}\pysigline{\sphinxbfcode{\sphinxupquote{\_product\_theta}}}
probability threshold for the product-classifier

\end{fulllineitems}

\index{\_shop\_clf (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker._shop_clf}}\pysigline{\sphinxbfcode{\sphinxupquote{\_shop\_clf}}}
shop classifier

\end{fulllineitems}

\index{\_food\_clf (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker._food_clf}}\pysigline{\sphinxbfcode{\sphinxupquote{\_food\_clf}}}
food classifier

\end{fulllineitems}

\index{\_product\_clf (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker._product_clf}}\pysigline{\sphinxbfcode{\sphinxupquote{\_product\_clf}}}
product classifier

\end{fulllineitems}

\index{\_p (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker._p}}\pysigline{\sphinxbfcode{\sphinxupquote{\_p}}}
preprocessor for websites

\end{fulllineitems}

\index{\_imp (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker._imp}}\pysigline{\sphinxbfcode{\sphinxupquote{\_imp}}}
impressums handler

\end{fulllineitems}

\index{\_extractor (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker._extractor}}\pysigline{\sphinxbfcode{\sphinxupquote{\_extractor}}}
information extractor

\end{fulllineitems}

\index{oeko (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker.oeko}}\pysigline{\sphinxbfcode{\sphinxupquote{oeko}}}
oeko module

\end{fulllineitems}

\index{health (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker.health}}\pysigline{\sphinxbfcode{\sphinxupquote{health}}}
health claim module

\end{fulllineitems}

\index{geo (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker.geo}}\pysigline{\sphinxbfcode{\sphinxupquote{geo}}}
geoschutz module

\end{fulllineitems}

\index{ingr (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker.ingr}}\pysigline{\sphinxbfcode{\sphinxupquote{ingr}}}
ingredients module

\end{fulllineitems}

\index{bioc (worker.Worker attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker.bioc}}\pysigline{\sphinxbfcode{\sphinxupquote{bioc}}}
bioc module

\end{fulllineitems}

\index{\_\_init\_\_() (worker.Worker method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{config}, \emph{delay\_module=180}}{}
Initialize all the needed modules.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{config} \textendash{} dictionary with connection details for db\_interface
and modules

\item[{Keyword Arguments}] \leavevmode
\sphinxstylestrong{delay\_module} \textendash{} time in seconds, after which each module is
killed

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_job\_and\_process() (worker.Worker method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker.get_job_and_process}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_job\_and\_process}}}{}{}
Get a job from database, calculate results, update database.

\end{fulllineitems}

\index{online\_training\_clfs() (worker.Worker method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker.online_training_clfs}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{online\_training\_clfs}}}{}{}
Online train the classifiers.

From the database all data, that was manually evaluated and
not yet used for training is selected. With a filter only
changed values are used to train the three classifiers. The
classifiers are then updated with online learning and all used
data is marked as such.

\end{fulllineitems}

\index{process\_job() (worker.Worker method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker.process_job}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{process\_job}}}{\emph{input\_file}, \emph{input\_image}, \emph{jobs\_str}}{}
Process a job and return the results of the single modules.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{input\_file} \textendash{} path to file that should be processed.
can be an url

\item {} 
\sphinxstylestrong{input\_image} \textendash{} path to a screenshot of the file

\item {} 
\sphinxstylestrong{jobs\_str} \textendash{} comma separated string with list of modules
that should be used

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{schedule() (worker.Worker method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker.schedule}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{schedule}}}{\emph{max\_tries}, \emph{delay}, \emph{day}, \emph{update\_rate}}{}
Register a worker for processing jobs.

{\hyperref[\detokenize{api:worker.Worker.get_job_and_process}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{get\_job\_and\_process()}}}}} is called periodically to process
all available jobs. If there is no job found, the worker
sleeps for a given amount of time before trying to get a job
again. Either after a specific number of tries the worker is
shut down or the worker tries always to get a new job a
specific number of times a day. At a given intervall the
database is checked for manually evaluated data that is newly
available.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{max\_tries} \textendash{} the maximal number of tries, the worker should
check for new jobs. if -1, the worker won’t stop checking

\item {} 
\sphinxstylestrong{delay} \textendash{} the time in seconds the worker should sleep between
checking for jobs

\item {} 
\sphinxstylestrong{day} \textendash{} number of times the worker should check for new jobs per
day. only used with max\_tries = -1

\item {} 
\sphinxstylestrong{update\_rate} \textendash{} number of days the worker should wait before
checking for newly available manually evaluated data

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{shutdown() (worker.Worker method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.Worker.shutdown}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{shutdown}}}{}{}
Shut down the worker and close connection to database.

\end{fulllineitems}


\end{fulllineitems}

\index{handler() (in module worker)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:worker.handler}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{worker.}}\sphinxbfcode{\sphinxupquote{handler}}}{\emph{signum}, \emph{frame}}{}
Handle too long executions of modules.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{signum} \textendash{} signal number, not used

\item {} 
\sphinxstylestrong{frame} \textendash{} not used

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}



\section{The Classifier (Shop, Food, Product):}
\label{\detokenize{api:the-classifier-shop-food-product}}\label{\detokenize{api:module-classifier}}\index{classifier (module)}\index{Classifier (class in classifier)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:classifier.Classifier}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{classifier.}}\sphinxbfcode{\sphinxupquote{Classifier}}}{\emph{config}, \emph{directory=None}, \emph{new=False}, \emph{type\_='shop'}}{}
A classification pipeline for different settings.

Classifier implements an abstract interface to the underlying SVM
for shop, food and product websites classification. It performs
feature selection, feature extraction and classification. The
different parameters for all three classifier instatiations are
choosen with cross-validation and are hard coded.
\index{pipeline (classifier.Classifier attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:classifier.Classifier.pipeline}}\pysigline{\sphinxbfcode{\sphinxupquote{pipeline}}}
pipeline of a TfidfVectorizer, a RandomForrest for
feature selection and a linear SVM.

\end{fulllineitems}

\index{type\_ (classifier.Classifier attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:classifier.Classifier.type_}}\pysigline{\sphinxbfcode{\sphinxupquote{type\_}}}
a string that describes the instantiation. Valid
strings: ‘shop’, ‘food’ and ‘product’

\end{fulllineitems}

\index{filename (classifier.Classifier attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:classifier.Classifier.filename}}\pysigline{\sphinxbfcode{\sphinxupquote{filename}}}
filename of the file where the pickled classifier
is stored.

\end{fulllineitems}

\index{prob (classifier.Classifier attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:classifier.Classifier.prob}}\pysigline{\sphinxbfcode{\sphinxupquote{prob}}}
an instance of \sphinxcode{\sphinxupquote{Probability}} that is trained on the same
data.

\end{fulllineitems}

\index{\_\_init\_\_() (classifier.Classifier method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:classifier.Classifier.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{config}, \emph{directory=None}, \emph{new=False}, \emph{type\_='shop'}}{}
Initialize the pipeline.

Constructor to initialize the pipeline. This function tries to load
the classifier from the file \sphinxcode{\sphinxupquote{self.filename}}. If this file does
not exist or new is True, a new classifier is trained and stored.

If the file \sphinxcode{\sphinxupquote{self.filename}} does not exist or new is True, and
directory is None the function ends with an error
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{config} \textendash{} dictionary with important configuration information

\item[{Keyword Arguments}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{directory} \textendash{} path to the directory with the training samples
(default: None)

\item {} 
\sphinxstylestrong{new} \textendash{} if True, a new classifier is trained with the set from
directory (default: False)

\item {} 
\sphinxstylestrong{type\_} \textendash{} specifies the type of the classifier. Valid strings:’shop’,
‘food’, ‘product’. (default: ‘shop’)

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{load\_files() (classifier.Classifier method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:classifier.Classifier.load_files}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{load\_files}}}{\emph{container\_path}}{}
Load the files in container\_path.

A specific structure of directories is assumed within
container\_path. The names of the subdirectories are taken as
target names and the data within the subdirectories is assumed
to be within different classes. The assumed structure is:
\begin{description}
\item[{container\_path/}] \leavevmode\begin{description}
\item[{0/}] \leavevmode
file1.html
file2.html
…

\item[{1/}] \leavevmode
file3.html
file4.html
…

\end{description}

\end{description}

All the files within the subdirectories are preprocessed with
BeautifulSoup. So HTML-files can directly be used as input for
training and testing purposes. BeautifulSoup handles plain
text files well, so that they can also be used for loading
with this function. Even directories with mixed types of text
files are possible.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{container\_path} \textendash{} path to the containing directory

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (classifier.Classifier method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:classifier.Classifier.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{data}}{}
Predict classes for data.

Predicts the class label for all data points in data.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{data} \textendash{} list of data points to be classified.

\end{description}\end{quote}

\end{fulllineitems}

\index{predict\_prob() (classifier.Classifier method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:classifier.Classifier.predict_prob}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict\_prob}}}{\emph{data}}{}
Predict the membership probability for the positive class.

Predicts the probability for all data points in data to be a
member of the positive class.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{data} \textendash{} a list of data points to be classified.

\end{description}\end{quote}

\end{fulllineitems}

\index{test() (classifier.Classifier method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:classifier.Classifier.test}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{test}}}{\emph{test\_data}, \emph{prob=False}}{}
Test \sphinxcode{\sphinxupquote{self.pipeline}}.

Tests the trained pipeline with the data from test\_data. After
predicting the label, the true positives, false positives,
false negatives, true negatives, precision, recall and
accuracy are calculated and printed on stdout.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{test\_data} \textendash{} path to the directory with the test data. The
same directory structure as for {\hyperref[\detokenize{api:classifier.Classifier.train}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{train()}}}}} is assumed.

\item[{Keyword Arguments}] \leavevmode
\sphinxstylestrong{prob} \textendash{} if True, {\hyperref[\detokenize{api:classifier.Classifier.predict_prob}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{predict\_prob()}}}}} is used to get the
classification instead of {\hyperref[\detokenize{api:classifier.Classifier.predict}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{predict()}}}}}. (default: False)

\end{description}\end{quote}

\end{fulllineitems}

\index{train() (classifier.Classifier method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:classifier.Classifier.train}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{training\_data}, \emph{config}}{}
Create and train a pipeline.

For Training the data in training\_data is used. After training
the pipeline is stored in the file \sphinxcode{\sphinxupquote{self.filename}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{training\_data} \textendash{} path to the directory with the training
data. A special directory structure is assumed: the directory
training\_data should contain two sub-directories ‘0’ and
‘1’. Sub-directory ‘0’ contains the positive examples and
sub-directory ‘1’ contains the negative examples (e.g. when
\sphinxcode{\sphinxupquote{self.type\_}} = ‘shop’, all shops are contained in ‘0’ and all
non-shops are contained in ‘1’).

\item {} 
\sphinxstylestrong{config} \textendash{} dictionary with necessary configuration details.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{train\_batch() (classifier.Classifier method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:classifier.Classifier.train_batch}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train\_batch}}}{\emph{data}, \emph{labels}}{}
Refine \sphinxcode{\sphinxupquote{self.pipeline}} with batch online learning.

This function is for online-learning the classifier with
e.g. manually labeled data. It only trains the classifier and
not the feature extraction, feature selection and probability
distribution part of the pipeline. For training data and
labels are used as a batch. After training the classifier,
the whole pipeline is stored in the file \sphinxcode{\sphinxupquote{self.filename}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{data} \textendash{} a list with the text from the manually labeled websites.

\item {} 
\sphinxstylestrong{labels} \textendash{} a list with the correspondin labels, that were
assigned by a human.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{train\_batch\_dir() (classifier.Classifier method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:classifier.Classifier.train_batch_dir}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train\_batch\_dir}}}{\emph{directory}}{}
Wrap {\hyperref[\detokenize{api:classifier.Classifier.train_batch}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{train\_batch()}}}}} to be used with directories.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{directory} \textendash{} path to directory with data.

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{api:module-probability}}\index{probability (module)}\index{Probability (class in probability)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:probability.Probability}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{probability.}}\sphinxbfcode{\sphinxupquote{Probability}}}{\emph{type\_}, \emph{pipe=None}, \emph{directory=None}, \emph{new=False}}{}
A Probability-Calculator for the output of a SVM.

This implementation follows the procedure in {[}1{]} with additional
changes from {[}2{]}.


\begin{fulllineitems}
\pysigline{\sphinxbfcode{\sphinxupquote{A,~B}}}
the calculated weights for the probability function:
P(class \textbar{} input) = 1 / ( 1 + exp(A * input + B))

\end{fulllineitems}


\begin{sphinxadmonition}{note}{Note:}\begin{description}
\item[{References:}] \leavevmode\begin{description}
\item[{{[}1{]} John C. Platt, Probabilistic Outputs for Support Vector}] \leavevmode
Machines and Comparison to Regularized Likelihood Methods, 2000

\item[{{[}2{]} Lin et al, A note on Platt’s probabilistic outputs for support}] \leavevmode
vector machines, 2007

\end{description}

\end{description}
\end{sphinxadmonition}
\index{\_\_init\_\_() (probability.Probability method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:probability.Probability.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{type\_}, \emph{pipe=None}, \emph{directory=None}, \emph{new=False}}{}
Initialize the Probability-Calculator.

Tries to read already trained parameters A and B from the file
“models/type\_\_sigmoid.txt” and expects them to be in the following
format:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A} \PYG{o}{\PYGZlt{}}\PYG{n}{number}\PYG{o}{\PYGZgt{}}
\PYG{n}{B} \PYG{o}{\PYGZlt{}}\PYG{n}{number}\PYG{o}{\PYGZgt{}}
\end{sphinxVerbatim}

if new is True or the file doesn’t exist, and pipe or
directory is None this function ends with an error.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{type\_} \textendash{} string that indicates the type of the
classifier. this string is used to identify the trained
sigmoidal function in the future

\item[{Keyword Arguments}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{pipe} \textendash{} pipeline to use to generate the training set (default:
None)

\item {} 
\sphinxstylestrong{directory} \textendash{} path to the directory of training samples
(default: None)

\item {} 
\sphinxstylestrong{new} \textendash{} if True, new parameters will be learned with the
training samples in directory (default: False)

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{calculate\_probability() (probability.Probability method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:probability.Probability.calculate_probability}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{calculate\_probability}}}{\emph{dec}}{}
Calculat the probability for dec to be in the positive class.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{dec} \textendash{} distance to the hyperplane for this data point

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_trainingset() (probability.Probability method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:probability.Probability.generate_trainingset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{generate\_trainingset}}}{\emph{pipeline}, \emph{directory}}{}
Generate training data fs and ys.

Generates a training set from the samples in directory
consisting of the distance to the hyperplane for all points
and their real labels (fs and ys). Therefore a 3fold
cross-validation with a SVM is performed. The feature
selection and parameters are not configurable.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{pipeline} \textendash{} pipeline of classifier, which is used to generate
the trainingset

\item {} 
\sphinxstylestrong{directory} \textendash{} path to the directory with the initial training samples

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{read\_from\_file() (probability.Probability method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:probability.Probability.read_from_file}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{read\_from\_file}}}{\emph{\_file\_}}{}
Read parameters A and B from \_file\_.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{\_file\_} \textendash{} filename of file to read from.

\end{description}\end{quote}

\end{fulllineitems}

\index{store\_to\_file() (probability.Probability method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:probability.Probability.store_to_file}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{store\_to\_file}}}{\emph{\_file\_}}{}
Store parameters A and B to \_file\_.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{\_file\_} \textendash{} filename of file to store to.

\end{description}\end{quote}

\end{fulllineitems}

\index{train\_probs() (probability.Probability method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:probability.Probability.train_probs}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train\_probs}}}{\emph{directory}}{}
Train the probability calculator.

Generates a training set from the samples in directory and
performs a minimization to learn the parameters A and B. The
new parameters are not stored in a file. Please use
{\hyperref[\detokenize{api:probability.Probability.store_to_file}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{store\_to\_file()}}}}} for this purpose. If the minimization is not
successfull, it is reported on stderr.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{directory} \textendash{} path to the directory with the initial training
samples

\end{description}\end{quote}

\end{fulllineitems}

\index{train\_probs\_orig() (probability.Probability method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:probability.Probability.train_probs_orig}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train\_probs\_orig}}}{\emph{pipe}, \emph{directory}}{}
Train the probability calculator.

Generates a training set from the samples in directory and
performs the original algorithm from {[}1{]} to learn the
parameters A and B. The new parameters are not stored in a
file. Please use {\hyperref[\detokenize{api:probability.Probability.store_to_file}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{store\_to\_file()}}}}} for this purpose. If the
minimization is not successfull, it is reported on stderr.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{directory} \textendash{} path to the directory with the initial training
samples

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{The Interface to the database:}
\label{\detokenize{api:module-db_interface}}\label{\detokenize{api:the-interface-to-the-database}}\index{db\_interface (module)}\index{DBInterface (class in db\_interface)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:db_interface.DBInterface}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{db\_interface.}}\sphinxbfcode{\sphinxupquote{DBInterface}}}{\emph{config=None}}{}
Interface to the used database scheme. Connects with the database
and performs all the needed queries.
\index{db (db\_interface.DBInterface attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:db_interface.DBInterface.db}}\pysigline{\sphinxbfcode{\sphinxupquote{db}}}
database connection object

\end{fulllineitems}

\index{\_\_init\_\_() (db\_interface.DBInterface method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:db_interface.DBInterface.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{config=None}}{}
Constructs a new Interface and connects to the database.
\begin{quote}\begin{description}
\item[{Keyword Arguments}] \leavevmode
\sphinxstylestrong{config} \textendash{} dictionary which contains important configuration
information (default: None)

\end{description}\end{quote}

\end{fulllineitems}

\index{connect() (db\_interface.DBInterface method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:db_interface.DBInterface.connect}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{connect}}}{\emph{config=None}}{}
Connects to the database.
\begin{quote}\begin{description}
\item[{Keyword Arguments}] \leavevmode
\sphinxstylestrong{config} \textendash{} dictionary which contains important configuration
information (default: None)

\end{description}\end{quote}

\end{fulllineitems}

\index{disconnect() (db\_interface.DBInterface method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:db_interface.DBInterface.disconnect}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{disconnect}}}{}{}
Close the open connection to the database.

\end{fulllineitems}

\index{get\_job() (db\_interface.DBInterface method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:db_interface.DBInterface.get_job}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_job}}}{}{}
Select one job from the database.

A query on the database is performed to get a new job with all needed
information. If no job is found, an empty list, else a list
with the needed information is returned.

\end{fulllineitems}

\index{get\_manual\_data() (db\_interface.DBInterface method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:db_interface.DBInterface.get_manual_data}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_manual\_data}}}{}{}
Select all rows where manual results have been altered.

Only rows, that weren’t used for online learning before are
selected here.

\end{fulllineitems}

\index{get\_results\_with\_parent\_resources() (db\_interface.DBInterface method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:db_interface.DBInterface.get_results_with_parent_resources}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_results\_with\_parent\_resources}}}{\emph{parent\_resources}}{}
Select the results from a already processed website.

The stored results for a main page can be retrieved with this
function. The result contains the keyword “add” where
additional information for all subpages is stored.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{parent\_resources} \textendash{} fk\_resources id for the main page

\end{description}\end{quote}

\end{fulllineitems}

\index{set\_error\_state() (db\_interface.DBInterface method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:db_interface.DBInterface.set_error_state}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{set\_error\_state}}}{\emph{id\_}}{}
Set status\_jobs to an error-flag for a given pk\_jobs.

For the given id status\_jobs is set to 9.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{id\_} \textendash{} pk\_jobs id

\end{description}\end{quote}

\end{fulllineitems}

\index{update\_results() (db\_interface.DBInterface method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:db_interface.DBInterface.update_results}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_results}}}{\emph{input\_}, \emph{results}}{}
Update the status and the result for one job.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{input\_} \textendash{} db information that was returned by {\hyperref[\detokenize{api:db_interface.DBInterface.get_job}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{get\_job()}}}}}.

\item {} 
\sphinxstylestrong{results} \textendash{} dictionary containing all results.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{update\_results\_with\_parent\_resources() (db\_interface.DBInterface method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:db_interface.DBInterface.update_results_with_parent_resources}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_results\_with\_parent\_resources}}}{\emph{pk\_results}, \emph{results}}{}
Update results for a given pk\_results id.

Sets both, analysis\_results and manual\_results, to the given
results dictionary.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{pk\_results} \textendash{} pk\_results id for a main page

\item {} 
\sphinxstylestrong{results} \textendash{} dictionary with all information to be stored

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{update\_update\_flag() (db\_interface.DBInterface method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:db_interface.DBInterface.update_update_flag}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_update\_flag}}}{\emph{rows}}{}
Update rows for being used in online-learning.

For all rows in rows the flag updated\_results is set to 1.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{rows} \textendash{} rows that have been used for online-learning

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Extraction of text from html:}
\label{\detokenize{api:extraction-of-text-from-html}}\label{\detokenize{api:module-preprocess}}\index{preprocess (module)}\index{Preprocessor (class in preprocess)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:preprocess.Preprocessor}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{preprocess.}}\sphinxbfcode{\sphinxupquote{Preprocessor}}}
Methods to preprocess html-files and text.
\index{beautiful\_soup() (preprocess.Preprocessor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:preprocess.Preprocessor.beautiful_soup}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{beautiful\_soup}}}{\emph{url}, \emph{links=True}}{}
Extract text and title of a webpage using beautiful soup.

With this function all the text and all links of a webpage are
extracted.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{url} \textendash{} url of the webpage or path to the file

\item[{Keyword Arguments}] \leavevmode
\sphinxstylestrong{links} \textendash{} if True, all links are extracted and added. (default: True)

\end{description}\end{quote}

\end{fulllineitems}

\index{preprocess\_file() (preprocess.Preprocessor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:preprocess.Preprocessor.preprocess_file}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{preprocess\_file}}}{\emph{file\_}, \emph{links=True}}{}
Extract all the text and title of a webpage.

Uses {\hyperref[\detokenize{api:preprocess.Preprocessor.beautiful_soup}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{beautiful\_soup()}}}}} to extract all the text, all links and
the title of a webpage.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{file\_} \textendash{} path to the webpage.

\item[{Keyword Arguments}] \leavevmode
\sphinxstylestrong{links} \textendash{} if True, all links are extracted as well. (default: True)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Extracting addresses:}
\label{\detokenize{api:extracting-addresses}}\label{\detokenize{api:module-impressum}}\index{impressum (module)}\index{Impressum\_handler (class in impressum)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum.Impressum_handler}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{impressum.}}\sphinxbfcode{\sphinxupquote{Impressum\_handler}}}{\emph{config}}{}
Extracts addresses from text.

Holds a trained Conditional Random Field to identify addresses in
text. The potential addresses are searched with a regular
expression, that matches 5-digit numbers followed by text, and
labeled with the CRF.
\index{regex (impressum.Impressum\_handler attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum.Impressum_handler.regex}}\pysigline{\sphinxbfcode{\sphinxupquote{regex}}}
a compiled regular expression to extract regions around a
postal code

\end{fulllineitems}

\index{imp (impressum.Impressum\_handler attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum.Impressum_handler.imp}}\pysigline{\sphinxbfcode{\sphinxupquote{imp}}}
a trained CRF which is accessible through ImpressumCRF

\end{fulllineitems}

\index{kr\_mapping (impressum.Impressum\_handler attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum.Impressum_handler.kr_mapping}}\pysigline{\sphinxbfcode{\sphinxupquote{kr\_mapping}}}
a mapping from postal codes to regions

\end{fulllineitems}

\index{\_\_init\_\_() (impressum.Impressum\_handler method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum.Impressum_handler.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{config}}{}
Initialize regex and load the trained CRF.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{config} \textendash{} dictionary, that contains configuration. The entry
with the key “map\_file” is interpreted as a csv file
containing a mapping from postal code to regions.

\end{description}\end{quote}

\end{fulllineitems}

\index{process\_text() (impressum.Impressum\_handler method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum.Impressum_handler.process_text}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{process\_text}}}{\emph{t}}{}
Search and return all addresses in t.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{t} \textendash{} the text to process.

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{api:module-impressum_crf}}\index{impressum\_crf (module)}\index{ImpressumCRF (class in impressum\_crf)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum_crf.ImpressumCRF}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{impressum\_crf.}}\sphinxbfcode{\sphinxupquote{ImpressumCRF}}}{\emph{cities='/home/dorle/Dokumente/data/osm\_germany/cities.txt'}, \emph{new=False}, \emph{x\_file=None}, \emph{y\_file=None}}{}
A sequence labeling algorithm for german street names using CRFs.
\index{cities (impressum\_crf.ImpressumCRF attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum_crf.ImpressumCRF.cities}}\pysigline{\sphinxbfcode{\sphinxupquote{cities}}}
set with all german city names (from wikipedia)

\end{fulllineitems}

\index{filename (impressum\_crf.ImpressumCRF attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum_crf.ImpressumCRF.filename}}\pysigline{\sphinxbfcode{\sphinxupquote{filename}}}
name for the file that is used to store the classifier

\end{fulllineitems}

\index{crf (impressum\_crf.ImpressumCRF attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum_crf.ImpressumCRF.crf}}\pysigline{\sphinxbfcode{\sphinxupquote{crf}}}
conditional random field to label sequences

\end{fulllineitems}

\index{\_\_init\_\_() (impressum\_crf.ImpressumCRF method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum_crf.ImpressumCRF.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{cities='/home/dorle/Dokumente/data/osm\_germany/cities.txt'}, \emph{new=False}, \emph{x\_file=None}, \emph{y\_file=None}}{}
Initialize the conditional random field.

If new is False, it tries to read the classifier from
\sphinxcode{\sphinxupquote{self.filename}}. If this file does not exist or new is True, a
new classifier is trained with the files x\_file and y\_file.

If new is True and x\_file or y\_file is None, the function ends
with an error.
\begin{quote}\begin{description}
\item[{Keyword Arguments}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{cities} \textendash{} filename of a city-file (default: city\_file)

\item {} 
\sphinxstylestrong{new} \textendash{} if True, a new classifier is trained (default: False)

\item {} 
\sphinxstylestrong{x\_file} \textendash{} file that contains the training sequences. Each
sequence has to be in a single line. (default: None)

\item {} 
\sphinxstylestrong{y\_file} \textendash{} file that contains the label sequences. The label
sequences have to be in the same line as the corresponding
training sequence in x\_file. (default: None)

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{create\_features() (impressum\_crf.ImpressumCRF method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum_crf.ImpressumCRF.create_features}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{create\_features}}}{\emph{word\_list}, \emph{pos}}{}
Create a feature vector for the word at pos in word\_list.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{word\_list} \textendash{} list of words

\item {} 
\sphinxstylestrong{pos} \textendash{} position in word\_list

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (impressum\_crf.ImpressumCRF method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum_crf.ImpressumCRF.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{samples}}{}
Predict the labels for every sample in samples.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{samples} \textendash{} list of samples. The samples have to be lists of
feature vectors.

\end{description}\end{quote}

\end{fulllineitems}

\index{seq2feat() (impressum\_crf.ImpressumCRF method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum_crf.ImpressumCRF.seq2feat}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{seq2feat}}}{\emph{seq}}{}
Create a list with feature vectors for every word in seq.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{seq} \textendash{} list of words

\end{description}\end{quote}

\end{fulllineitems}

\index{test() (impressum\_crf.ImpressumCRF method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum_crf.ImpressumCRF.test}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{test}}}{\emph{x\_test}, \emph{y\_test}}{}
Test the \sphinxcode{\sphinxupquote{self.crf}}.

For testing the test\_data from x\_test and y\_test is used. This
function prints a metric to stdout.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{x\_test} \textendash{} list with the test sequences as list of words

\item {} 
\sphinxstylestrong{y\_test} \textendash{} list with the label sequences as list of words. The
label sequences have to correspond with the test sequences in
x\_test.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{train() (impressum\_crf.ImpressumCRF method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:impressum_crf.ImpressumCRF.train}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{x\_list}, \emph{y\_list}}{}
Train a conditional random field.

For Training the CRF the samples from x\_list and y\_list are
used. After training the CRF is stored to
\sphinxcode{\sphinxupquote{self.filename}}. \sphinxcode{\sphinxupquote{self.crf}} is changed by this method.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{x\_list} \textendash{} list with the training sequences as list of words

\item {} 
\sphinxstylestrong{y\_list} \textendash{} list with the label sequences as list of words. The
label sequences have to correspond with the training sequences
in x\_list.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Extracting relevant product information:}
\label{\detokenize{api:module-information_extractor}}\label{\detokenize{api:extracting-relevant-product-information}}\index{information\_extractor (module)}\index{InformationExtractor (class in information\_extractor)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:information_extractor.InformationExtractor}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{information\_extractor.}}\sphinxbfcode{\sphinxupquote{InformationExtractor}}}
Functionality to extract information from product websites.
\index{anr (information\_extractor.InformationExtractor attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:information_extractor.InformationExtractor.anr}}\pysigline{\sphinxbfcode{\sphinxupquote{anr}}}
extractor for “Artikelnummern”

\end{fulllineitems}

\index{pr\_name (information\_extractor.InformationExtractor attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:information_extractor.InformationExtractor.pr_name}}\pysigline{\sphinxbfcode{\sphinxupquote{pr\_name}}}
crf to extract product names from webpage titles

\end{fulllineitems}

\index{\_\_init\_\_() (information\_extractor.InformationExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:information_extractor.InformationExtractor.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{}{}
Initialize all extractors.

\end{fulllineitems}

\index{extract\_artikelnummer() (information\_extractor.InformationExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:information_extractor.InformationExtractor.extract_artikelnummer}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{extract\_artikelnummer}}}{\emph{text}}{}
Extract “Artikelnummern” from text.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{text} \textendash{} some text

\end{description}\end{quote}

\end{fulllineitems}

\index{extract\_productname() (information\_extractor.InformationExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:information_extractor.InformationExtractor.extract_productname}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{extract\_productname}}}{\emph{title}}{}
Extract productnames from webpage titles.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{title} \textendash{} title from a webpage

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{api:module-anr_matcher}}\index{anr\_matcher (module)}\index{ANRMatcher (class in anr\_matcher)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:anr_matcher.ANRMatcher}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{anr\_matcher.}}\sphinxbfcode{\sphinxupquote{ANRMatcher}}}
A regular expression wrapper to match ‘Artikelnummern’.
\index{complete\_re (anr\_matcher.ANRMatcher attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:anr_matcher.ANRMatcher.complete_re}}\pysigline{\sphinxbfcode{\sphinxupquote{complete\_re}}}
regular expression that captures some common forms
for ‘Artikelnummern’

\end{fulllineitems}

\index{\_\_init\_\_() (anr\_matcher.ANRMatcher method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:anr_matcher.ANRMatcher.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{}{}
Initialize the regular expression wrapper.

\end{fulllineitems}

\index{match() (anr\_matcher.ANRMatcher method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:anr_matcher.ANRMatcher.match}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{match}}}{\emph{t}}{}
Match t with the regular expression.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{t} \textendash{} the text, against which \sphinxcode{\sphinxupquote{self.complete\_re}} shall be
matched.

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{api:module-product_crf}}\index{product\_crf (module)}\index{ProductnameCRF (class in product\_crf)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:product_crf.ProductnameCRF}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{product\_crf.}}\sphinxbfcode{\sphinxupquote{ProductnameCRF}}}{\emph{new=False}, \emph{x\_file=None}, \emph{y\_file=None}, \emph{cros\_val=False}}{}
A sequence labeling algorithm for product names using CRFs.
\index{filename (product\_crf.ProductnameCRF attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:product_crf.ProductnameCRF.filename}}\pysigline{\sphinxbfcode{\sphinxupquote{filename}}}
name for the file that is used to store the classifier

\end{fulllineitems}

\index{crf (product\_crf.ProductnameCRF attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:product_crf.ProductnameCRF.crf}}\pysigline{\sphinxbfcode{\sphinxupquote{crf}}}
conditional random field to label sequences

\end{fulllineitems}

\index{\_\_init\_\_() (product\_crf.ProductnameCRF method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:product_crf.ProductnameCRF.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{new=False}, \emph{x\_file=None}, \emph{y\_file=None}, \emph{cros\_val=False}}{}
Initialize the conditional random field.

If new is False, it tries to read the classifier from
\sphinxcode{\sphinxupquote{self.filename}}. If this file does not exist or new is True, a
new classifier is trained with the files x\_file and y\_file.

If new is True and x\_file or y\_file is None, the function ends
with an error.
\begin{quote}\begin{description}
\item[{Keyword Arguments}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{new} \textendash{} if True, a new classifier is trained (default:false)

\item {} 
\sphinxstylestrong{x\_file} \textendash{} file that contains the training sequences. Each
sequence has to be in a single line. (default: None)

\item {} 
\sphinxstylestrong{y\_file} \textendash{} file that contains the label sequences. The label
sequences have to be in the same line as the corresponding
training sequence in x\_file. (default: None)

\item {} 
\sphinxstylestrong{cros\_val} \textendash{} default: False

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{create\_features() (product\_crf.ProductnameCRF method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:product_crf.ProductnameCRF.create_features}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{create\_features}}}{\emph{word\_list}, \emph{pos}, \emph{l}, \emph{title=set({[}{]})}}{}
Create a feature dictionary for a given word.

The feature dictionary is build for the word at position pos
in word\_list.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{word\_list} \textendash{} list of words, for which a sequence of feature
dictionaries should be created.

\item {} 
\sphinxstylestrong{pos} \textendash{} position of the current word in word\_list

\item {} 
\sphinxstylestrong{l} \textendash{} length of word\_list

\end{itemize}

\item[{Keyword Arguments}] \leavevmode
\sphinxstylestrong{title} \textendash{} a set with all common tokens in titles of websites
(default: set())

\end{description}\end{quote}

\end{fulllineitems}

\index{load\_files() (product\_crf.ProductnameCRF method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:product_crf.ProductnameCRF.load_files}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{load\_files}}}{\emph{x\_file}, \emph{y\_file}}{}
Load training data from files.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{x\_file} \textendash{} file that contains the training sequences. Each
sequence has to be in a single line.

\item {} 
\sphinxstylestrong{y\_file} \textendash{} file that contains the label sequences. The label
sequences have to be in the same line as the corresponding
training sequence in x\_file.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (product\_crf.ProductnameCRF method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:product_crf.ProductnameCRF.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{samples}}{}
Predict the labels for samples.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{samples} \textendash{} list of sequences, for which the labels shall be
predicted

\end{description}\end{quote}

\end{fulllineitems}

\index{seq2feat() (product\_crf.ProductnameCRF method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:product_crf.ProductnameCRF.seq2feat}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{seq2feat}}}{\emph{seq}}{}
Create a list with feature dictionaries for seq.

A feature dictionary for every word in seq is created with the
method {\hyperref[\detokenize{api:product_crf.ProductnameCRF.create_features}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{create\_features()}}}}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{seq} \textendash{} list with words, for which a sequence of feature
dictionaries should be build

\end{description}\end{quote}

\end{fulllineitems}

\index{test() (product\_crf.ProductnameCRF method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:product_crf.ProductnameCRF.test}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{test}}}{\emph{x\_test}, \emph{y\_test}}{}
Test the conditional random field.

The results are printed to stdout.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{x\_test} \textendash{} list with test sequences

\item {} 
\sphinxstylestrong{y\_test} \textendash{} list with test labels. The labels have to be at the
same index as the corresponding sequences.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{train() (product\_crf.ProductnameCRF method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:product_crf.ProductnameCRF.train}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{x\_list}, \emph{y\_list}}{}
Train a conditional random field.

The trained conditional random field is stored in
\sphinxcode{\sphinxupquote{self.filename}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{x\_list} \textendash{} list with all training sequences

\item {} 
\sphinxstylestrong{y\_list} \textendash{} list with all training labels. The labels have to be
at the same index as the corresponding sequences.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Extracting text information about ecological traders:}
\label{\detokenize{api:module-oeko}}\label{\detokenize{api:extracting-text-information-about-ecological-traders}}\index{oeko (module)}\index{Oeko (class in oeko)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:oeko.Oeko}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{oeko.}}\sphinxbfcode{\sphinxupquote{Oeko}}}{\emph{config}}{}
Functionality to check ‘Biohändler’ for correct labelling.
\index{buzz\_reg (oeko.Oeko attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:oeko.Oeko.buzz_reg}}\pysigline{\sphinxbfcode{\sphinxupquote{buzz\_reg}}}
regular expression matching special buzzwords

\end{fulllineitems}

\index{num\_reg (oeko.Oeko attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:oeko.Oeko.num_reg}}\pysigline{\sphinxbfcode{\sphinxupquote{num\_reg}}}
regular expression matching german ‘Ökonummern’

\end{fulllineitems}

\index{legal\_numbers (oeko.Oeko attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:oeko.Oeko.legal_numbers}}\pysigline{\sphinxbfcode{\sphinxupquote{legal\_numbers}}}
list of legal ‘Ökonummern’

\end{fulllineitems}

\index{\_\_init\_\_() (oeko.Oeko method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:oeko.Oeko.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{config}}{}
Initialize attributes.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{config} \textendash{} dictionary with important configuration information.

\end{description}\end{quote}

\end{fulllineitems}

\index{check\_image() (oeko.Oeko method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:oeko.Oeko.check_image}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{check\_image}}}{\emph{image\_name}}{}
Check a screenshot for the mandatory eu logo.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{image\_name} \textendash{} path to the screenshot

\end{description}\end{quote}

\end{fulllineitems}

\index{check\_text() (oeko.Oeko method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:oeko.Oeko.check_text}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{check\_text}}}{\emph{text}}{}
Check text for buzzwords and ‘Ökonummern’.

The found ‘Ökonummern’ are verified if they are legal. Illegal
‘Ökonummern’ are reported also.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{text} \textendash{} text to check

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{EU Logo recognition module:}
\label{\detokenize{api:eu-logo-recognition-module}}\label{\detokenize{api:module-logos}}\index{logos (module)}\phantomsection\label{\detokenize{api:module-find_logos}}\index{find\_logos (module)}\index{find\_logos() (in module find\_logos)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:find_logos.find_logos}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{find\_logos.}}\sphinxbfcode{\sphinxupquote{find\_logos}}}{\emph{image\_name}}{}
Find logos in an image.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{image\_name} \textendash{} name of image

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_train\_snippets\_and\_store\_them() (in module find\_logos)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:find_logos.generate_train_snippets_and_store_them}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{find\_logos.}}\sphinxbfcode{\sphinxupquote{generate\_train\_snippets\_and\_store\_them}}}{\emph{image\_list}, \emph{file\_name}}{}
Generate snippets for training.

The generated snippets are stored into countour\_name.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{image\_list} \textendash{} list with image names

\item {} 
\sphinxstylestrong{file\_name} \textendash{} not used

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_shapes() (in module find\_logos)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:find_logos.get_shapes}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{find\_logos.}}\sphinxbfcode{\sphinxupquote{get\_shapes}}}{\emph{img}}{}
Calculate the contours of an image.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{img} \textendash{} already loaded image

\end{description}\end{quote}

\end{fulllineitems}

\index{intersect() (in module find\_logos)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:find_logos.intersect}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{find\_logos.}}\sphinxbfcode{\sphinxupquote{intersect}}}{\emph{r}, \emph{pos}}{}
Intersect two rectangles.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{r} \textendash{} first rectangle

\item {} 
\sphinxstylestrong{pos} \textendash{} second rectangle

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{load\_positions() (in module find\_logos)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:find_logos.load_positions}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{find\_logos.}}\sphinxbfcode{\sphinxupquote{load\_positions}}}{\emph{file\_name}}{}
Load example positions.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{file\_name} \textendash{} path to file

\end{description}\end{quote}

\end{fulllineitems}

\phantomsection\label{\detokenize{api:module-color_filter2}}\index{color\_filter2 (module)}\index{area() (in module color\_filter2)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:color_filter2.area}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{color\_filter2.}}\sphinxbfcode{\sphinxupquote{area}}}{\emph{r}}{}
Calculate area of rectangle.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{r} \textendash{} rectangle

\end{description}\end{quote}

\end{fulllineitems}

\index{color\_filter() (in module color\_filter2)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:color_filter2.color_filter}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{color\_filter2.}}\sphinxbfcode{\sphinxupquote{color\_filter}}}{\emph{img}}{}
Apply color filter to img.

Returns all rectangles within which the color is according to the
color filter.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{img} \textendash{} image in opencv image data type

\end{description}\end{quote}

\end{fulllineitems}

\index{is\_rect\_shape() (in module color\_filter2)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:color_filter2.is_rect_shape}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{color\_filter2.}}\sphinxbfcode{\sphinxupquote{is\_rect\_shape}}}{\emph{c}}{}
Check if contour is approximately rectangular.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{c} \textendash{} contour

\end{description}\end{quote}

\end{fulllineitems}

\index{merge\_rects() (in module color\_filter2)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:color_filter2.merge_rects}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{color\_filter2.}}\sphinxbfcode{\sphinxupquote{merge\_rects}}}{\emph{rects}}{}
Merge intersecting rectangles.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{rects} \textendash{} list of rectangles

\end{description}\end{quote}

\end{fulllineitems}

\index{ratio() (in module color\_filter2)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:color_filter2.ratio}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{color\_filter2.}}\sphinxbfcode{\sphinxupquote{ratio}}}{\emph{r}}{}
Calculate width to height ratio.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{r} \textendash{} rectangle

\end{description}\end{quote}

\end{fulllineitems}



\section{Detecting possible health claims:}
\label{\detokenize{api:module-health_claims}}\label{\detokenize{api:detecting-possible-health-claims}}\index{health\_claims (module)}\index{HealthClaims (class in health\_claims)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{health\_claims.}}\sphinxbfcode{\sphinxupquote{HealthClaims}}}{\emph{config}}{}
Different strategies to check for not allowed health claims.

The first strategy assumes a list with simple substances and a
list with diseases (default: ‘./health\_claim\_substances.txt’
and ‘./health\_claim\_diseases.txt’). The text of all websites
registered for this module is searched for every line in these
lists. When one or more diseases are found in the text, the found
substances and diseases are returned, while just a substance is
not enough for a suspicion.

The second strategy searches for all health claims in a
list. Therefore a list with prohibited health claims has to be
provided (default: ‘./rejected\_claims.txt’). In this file every
line is interpreted as a health claim and searched for.

The third strategy extracts all the relations from the text. Only
relations with a verb phrase contained in a provided file
(default: ‘./vps.txt’) are returned.
\index{pattern\_matcher\_sub (health\_claims.HealthClaims attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.pattern_matcher_sub}}\pysigline{\sphinxbfcode{\sphinxupquote{pattern\_matcher\_sub}}}
a simple pattern matcher that can search
for substances

\end{fulllineitems}

\index{pattern\_matcher\_dis (health\_claims.HealthClaims attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.pattern_matcher_dis}}\pysigline{\sphinxbfcode{\sphinxupquote{pattern\_matcher\_dis}}}
a simple pattern matcher that can search
for disease

\end{fulllineitems}

\index{pattern\_matcher\_fix (health\_claims.HealthClaims attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.pattern_matcher_fix}}\pysigline{\sphinxbfcode{\sphinxupquote{pattern\_matcher\_fix}}}
simple pattern matcher for fix health
claims

\end{fulllineitems}

\index{sub (health\_claims.HealthClaims attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.sub}}\pysigline{\sphinxbfcode{\sphinxupquote{sub}}}
list with relevant substances

\end{fulllineitems}

\index{dis (health\_claims.HealthClaims attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.dis}}\pysigline{\sphinxbfcode{\sphinxupquote{dis}}}
list with relevant diseases

\end{fulllineitems}

\index{verbs (health\_claims.HealthClaims attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.verbs}}\pysigline{\sphinxbfcode{\sphinxupquote{verbs}}}
set with relevant verbs

\end{fulllineitems}

\index{pos (health\_claims.HealthClaims attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.pos}}\pysigline{\sphinxbfcode{\sphinxupquote{pos}}}
Part-of-Speech-Tagger

\end{fulllineitems}

\index{punkt\_name (health\_claims.HealthClaims attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.punkt_name}}\pysigline{\sphinxbfcode{\sphinxupquote{punkt\_name}}}
filename of pretrained sentence tokenizer

\end{fulllineitems}

\index{punkt (health\_claims.HealthClaims attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.punkt}}\pysigline{\sphinxbfcode{\sphinxupquote{punkt}}}
pretrained sentence tokenizer

\end{fulllineitems}

\index{\_\_init\_\_() (health\_claims.HealthClaims method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{config}}{}
Initialize all the matchers.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{config} \textendash{} dictionary with important configuration information.

\end{description}\end{quote}

\end{fulllineitems}

\index{check\_disease\_substances() (health\_claims.HealthClaims method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.check_disease_substances}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{check\_disease\_substances}}}{\emph{text}}{}
Execute the first strategy.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{text} \textendash{} the text that should be searched through

\end{description}\end{quote}

\end{fulllineitems}

\index{check\_fix\_patterns() (health\_claims.HealthClaims method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.check_fix_patterns}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{check\_fix\_patterns}}}{\emph{text}}{}
Execute the second strategy.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{text} \textendash{} the text that should be searched through

\end{description}\end{quote}

\end{fulllineitems}

\index{check\_semantic\_relations() (health\_claims.HealthClaims method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.check_semantic_relations}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{check\_semantic\_relations}}}{\emph{text}}{}
Execute the third strategy.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{text} \textendash{} the text that shoulb be searched

\end{description}\end{quote}

\end{fulllineitems}

\index{chunks() (health\_claims.HealthClaims method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:health_claims.HealthClaims.chunks}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{chunks}}}{\emph{l}, \emph{n}}{}
Yield successive n-sized chunks from l.

\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{api:module-pos_tagger}}\index{pos\_tagger (module)}\index{POSTagger (class in pos\_tagger)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:pos_tagger.POSTagger}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{pos\_tagger.}}\sphinxbfcode{\sphinxupquote{POSTagger}}}{\emph{model\_file='models/health\_claim\_model.crf.tagger'}}{}
Simple POS-Tagger using conditional random fields.
\index{tagger (pos\_tagger.POSTagger attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:pos_tagger.POSTagger.tagger}}\pysigline{\sphinxbfcode{\sphinxupquote{tagger}}}
the pretrained tagger

\end{fulllineitems}

\index{\_\_init\_\_() (pos\_tagger.POSTagger method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:pos_tagger.POSTagger.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{model\_file='models/health\_claim\_model.crf.tagger'}}{}
Initialize tagger.
\begin{quote}\begin{description}
\item[{Keyword Arguments}] \leavevmode
\sphinxstylestrong{model\_file} \textendash{} path to pretrained model
(default: ./models/health\_claim\_model.crf.tagger’)

\end{description}\end{quote}

\end{fulllineitems}

\index{pos\_tag\_io() (pos\_tagger.POSTagger method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:pos_tagger.POSTagger.pos_tag_io}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{pos\_tag\_io}}}{}{}
Read from stdin, tag and write to stdout.

\end{fulllineitems}

\index{pos\_tag\_lst() (pos\_tagger.POSTagger method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:pos_tagger.POSTagger.pos_tag_lst}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{pos\_tag\_lst}}}{\emph{lst}, \emph{output}}{}
Tag every word of every sentence in lst and write to output.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{lst} \textendash{} list of sentences

\item {} 
\sphinxstylestrong{output} \textendash{} filehandle

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{api:module-conll_parser}}\index{conll\_parser (module)}\index{Entry (class in conll\_parser)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Entry}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{conll\_parser.}}\sphinxbfcode{\sphinxupquote{Entry}}}{\emph{id\_}, \emph{form}, \emph{pos}, \emph{xpos}, \emph{head}, \emph{dep}}{}
Contains important information for a conll entry
\index{id\_ (conll\_parser.Entry attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Entry.id_}}\pysigline{\sphinxbfcode{\sphinxupquote{id\_}}}
id of the entry

\end{fulllineitems}

\index{form (conll\_parser.Entry attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Entry.form}}\pysigline{\sphinxbfcode{\sphinxupquote{form}}}
actual word

\end{fulllineitems}

\index{pos (conll\_parser.Entry attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Entry.pos}}\pysigline{\sphinxbfcode{\sphinxupquote{pos}}}
POS-tag

\end{fulllineitems}

\index{xpos (conll\_parser.Entry attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Entry.xpos}}\pysigline{\sphinxbfcode{\sphinxupquote{xpos}}}
STTS-POS-tag

\end{fulllineitems}

\index{head (conll\_parser.Entry attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Entry.head}}\pysigline{\sphinxbfcode{\sphinxupquote{head}}}
id of parent node

\end{fulllineitems}

\index{dep (conll\_parser.Entry attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Entry.dep}}\pysigline{\sphinxbfcode{\sphinxupquote{dep}}}
relationship to parent node

\end{fulllineitems}

\index{\_\_init\_\_() (conll\_parser.Entry method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Entry.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{id\_}, \emph{form}, \emph{pos}, \emph{xpos}, \emph{head}, \emph{dep}}{}
Initialize entry.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{id\_} \textendash{} id of entry

\item {} 
\sphinxstylestrong{form} \textendash{} actual word

\item {} 
\sphinxstylestrong{pos} \textendash{} POS-tag

\item {} 
\sphinxstylestrong{xpos} \textendash{} STTS-POS-tag

\item {} 
\sphinxstylestrong{head} \textendash{} id of parent node

\item {} 
\sphinxstylestrong{dep} \textendash{} relationship to parent node

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Node (class in conll\_parser)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Node}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{conll\_parser.}}\sphinxbfcode{\sphinxupquote{Node}}}{\emph{data}}{}
Node for conll parser tree.
\index{data (conll\_parser.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Node.data}}\pysigline{\sphinxbfcode{\sphinxupquote{data}}}
data of the represented row

\end{fulllineitems}

\index{lchilds (conll\_parser.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Node.lchilds}}\pysigline{\sphinxbfcode{\sphinxupquote{lchilds}}}
list of left childs, sorted by \sphinxcode{\sphinxupquote{self.data.id\_}}

\end{fulllineitems}

\index{rchilds (conll\_parser.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Node.rchilds}}\pysigline{\sphinxbfcode{\sphinxupquote{rchilds}}}
list of right childs, sorted by \sphinxcode{\sphinxupquote{self.data.id\_}}

\end{fulllineitems}

\index{\_\_init\_\_() (conll\_parser.Node method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Node.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{data}}{}
\end{fulllineitems}

\index{add\_child() (conll\_parser.Node method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Node.add_child}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{add\_child}}}{\emph{id\_}}{}
Add a child to this node.

This function assumes, that the actual node is already
initialized. Only the id is added to one of the list of
children.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{id\_} \textendash{} id of the node to add

\end{description}\end{quote}

\end{fulllineitems}

\index{add\_lchild() (conll\_parser.Node method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Node.add_lchild}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{add\_lchild}}}{\emph{id\_}}{}
Add a left child to this node.

This function assumes, that the actual node is already
initialized. Only the id is added to the list of left
children.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{id\_} \textendash{} id of the node to add

\end{description}\end{quote}

\end{fulllineitems}

\index{add\_rchild() (conll\_parser.Node method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Node.add_rchild}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{add\_rchild}}}{\emph{id\_}}{}
Add a right child to this node.

This function assumes, that the actual node is already
initialized. Only the id is added to the list of right
children.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{id\_} \textendash{} id of the node to add

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_children() (conll\_parser.Node method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Node.get_children}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_children}}}{}{}
Return a list with all children.

\end{fulllineitems}

\index{get\_dep() (conll\_parser.Node method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Node.get_dep}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_dep}}}{}{}
Return the dependency relation retrieved from \sphinxcode{\sphinxupquote{self.data}}.

\end{fulllineitems}


\end{fulllineitems}

\index{Tree (class in conll\_parser)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Tree}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{conll\_parser.}}\sphinxbfcode{\sphinxupquote{Tree}}}{\emph{nodes}}{}
Dependency tree for one sentence.

This structure models the dependency relations of single phrases
found with an dependency parser. It is constructed from the
information in the conll format.
\index{root (conll\_parser.Tree attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Tree.root}}\pysigline{\sphinxbfcode{\sphinxupquote{root}}}
id of root node (always 0)

\end{fulllineitems}

\index{nodes (conll\_parser.Tree attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Tree.nodes}}\pysigline{\sphinxbfcode{\sphinxupquote{nodes}}}
list of nodes

\end{fulllineitems}

\index{\_\_init\_\_() (conll\_parser.Tree method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Tree.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{nodes}}{}
Initialize tree.

This function only adds all nodes without dependencies to the
tree. The structure has to be formed with
{\hyperref[\detokenize{api:conll_parser.Tree.create_tree_structure}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{create\_tree\_structure()}}}}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{nodes} \textendash{} list of entries

\end{description}\end{quote}

\end{fulllineitems}

\index{create\_tree\_structure() (conll\_parser.Tree method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Tree.create_tree_structure}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{create\_tree\_structure}}}{\emph{head\_indexed}}{}
Create the dependency structure.

This function creates the given dependency relationship
between single nodes within the tree.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{head\_indexed} \textendash{} dictionary mapping id to a list of children

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_leftest\_child() (conll\_parser.Tree method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Tree.get_leftest_child}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_leftest\_child}}}{\emph{id\_}}{}
Return leftest child under a node.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{id\_} \textendash{} id of the node

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_rightest\_child() (conll\_parser.Tree method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Tree.get_rightest_child}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_rightest\_child}}}{\emph{id\_}}{}
Return rightest child under a node.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{id\_} \textendash{} id of the node

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_rightest\_noun\_part() (conll\_parser.Tree method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Tree.get_rightest_noun_part}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_rightest\_noun\_part}}}{\emph{id\_}}{}
Return the rightest part of a noun phrase.

Expands the noun phrase starting at a node heuristically to
the right.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{id\_} \textendash{} id of the node

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_subtree() (conll\_parser.Tree method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Tree.get_subtree}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_subtree}}}{\emph{id\_}}{}
Return subtree under a node.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{id\_} \textendash{} id of the root for the subtree

\end{description}\end{quote}

\end{fulllineitems}

\index{string\_from\_to() (conll\_parser.Tree method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.Tree.string_from_to}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{string\_from\_to}}}{\emph{i}, \emph{j}}{}
Return the string in the intervall from i to j.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{i} \textendash{} id of left border

\item {} 
\sphinxstylestrong{j} \textendash{} id of right border

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{get\_relation() (in module conll\_parser)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.get_relation}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{conll\_parser.}}\sphinxbfcode{\sphinxupquote{get\_relation}}}{\emph{t}}{}
Splits tree at root and returns the relevant strings.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{t} \textendash{} tree to split

\end{description}\end{quote}

\end{fulllineitems}

\index{parse() (in module conll\_parser)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.parse}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{conll\_parser.}}\sphinxbfcode{\sphinxupquote{parse}}}{\emph{text}}{}
Parse text and return a list of lists.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{text} \textendash{} conll formated text for possibly more than one sentence

\end{description}\end{quote}

\end{fulllineitems}

\index{parse\_int\_value() (in module conll\_parser)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.parse_int_value}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{conll\_parser.}}\sphinxbfcode{\sphinxupquote{parse\_int\_value}}}{\emph{value}}{}
Parse int from string handling malformed ints.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{value} \textendash{} value to parse

\end{description}\end{quote}

\end{fulllineitems}

\index{parse\_line() (in module conll\_parser)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.parse_line}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{conll\_parser.}}\sphinxbfcode{\sphinxupquote{parse\_line}}}{\emph{line}}{}
Parse one line of conll format and return Entry.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{line} \textendash{} one line of conll format

\end{description}\end{quote}

\end{fulllineitems}

\index{parse\_tree() (in module conll\_parser)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.parse_tree}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{conll\_parser.}}\sphinxbfcode{\sphinxupquote{parse\_tree}}}{\emph{text}}{}
Parse text into dependency trees.

A list of trees is created containing one tree per sentence.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{text} \textendash{} conll formated text for possibly more than one sentence

\end{description}\end{quote}

\end{fulllineitems}

\index{split\_tree() (in module conll\_parser)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:conll_parser.split_tree}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{conll\_parser.}}\sphinxbfcode{\sphinxupquote{split\_tree}}}{\emph{t}}{}
Split tree at root node into relevant phrases.

This function returns the intervalls of the phrases.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{t} \textendash{} tree to split

\end{description}\end{quote}

\end{fulllineitems}



\section{Textanalysis for protected designation of geographical origin:}
\label{\detokenize{api:module-geoschutz_check}}\label{\detokenize{api:textanalysis-for-protected-designation-of-geographical-origin}}\index{geoschutz\_check (module)}\index{Geoschutz (class in geoschutz\_check)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:geoschutz_check.Geoschutz}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{geoschutz\_check.}}\sphinxbfcode{\sphinxupquote{Geoschutz}}}{\emph{config}}{}
Functionality to detect EU-certificated product names.
\index{s (geoschutz\_check.Geoschutz attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:geoschutz_check.Geoschutz.s}}\pysigline{\sphinxbfcode{\sphinxupquote{s}}}
dictionary with the keys ‘PGI’, ‘PDO’ and ‘TSG’. For each key
all the certificated productnames are stored in a set.

\end{fulllineitems}

\index{stemmer (geoschutz\_check.Geoschutz attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:geoschutz_check.Geoschutz.stemmer}}\pysigline{\sphinxbfcode{\sphinxupquote{stemmer}}}
a german stemmer

\end{fulllineitems}

\index{stop\_words (geoschutz\_check.Geoschutz attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:geoschutz_check.Geoschutz.stop_words}}\pysigline{\sphinxbfcode{\sphinxupquote{stop\_words}}}
a set with german stopwords

\end{fulllineitems}

\index{max\_n (geoschutz\_check.Geoschutz attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:geoschutz_check.Geoschutz.max_n}}\pysigline{\sphinxbfcode{\sphinxupquote{max\_n}}}
the length of the longest productname contained in s

\end{fulllineitems}

\index{compound\_reg (geoschutz\_check.Geoschutz attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:geoschutz_check.Geoschutz.compound_reg}}\pysigline{\sphinxbfcode{\sphinxupquote{compound\_reg}}}
regular expression with common delimiters for
compound nouns

\end{fulllineitems}

\index{\_\_init\_\_() (geoschutz\_check.Geoschutz method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:geoschutz_check.Geoschutz.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{config}}{}
Initialize all attributes.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{config} \textendash{} dictionary with important configuration information

\end{description}\end{quote}

\end{fulllineitems}

\index{create\_ngrams() (geoschutz\_check.Geoschutz method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:geoschutz_check.Geoschutz.create_ngrams}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{create\_ngrams}}}{\emph{lst}, \emph{min\_n=1}, \emph{max\_n=1}}{}
Create n-grams from a list.

All n-grams for n in {[}min\_n, max\_n{]} are created.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{lst} \textendash{} list of words

\item[{Keyword Arguments}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{min\_n} \textendash{} minimal length of an n-gram (default: 1)

\item {} 
\sphinxstylestrong{max\_n} \textendash{} maximal length of an n-gram (default: 1)

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{remove\_stop\_words() (geoschutz\_check.Geoschutz method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:geoschutz_check.Geoschutz.remove_stop_words}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{remove\_stop\_words}}}{\emph{lst}}{}
Remove german stop words.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{lst} \textendash{} list with words

\end{description}\end{quote}

\end{fulllineitems}

\index{search() (geoschutz\_check.Geoschutz method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:geoschutz_check.Geoschutz.search}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{search}}}{\emph{word}}{}
Search a productname in all the certificated productnames.

For the search word is normalized in the same ways, as all the
productnames in \sphinxcode{\sphinxupquote{self.s}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{word} \textendash{} productname to search for in all certificated
productnames.

\end{description}\end{quote}

\end{fulllineitems}

\index{stem\_all\_words() (geoschutz\_check.Geoschutz method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:geoschutz_check.Geoschutz.stem_all_words}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stem\_all\_words}}}{\emph{lst}}{}
Stem all words.

A german stemmer is used, so words from other languages may be
stemmed incorrectly. The result can be compared to other
stemmed words, because the procedure is deterministic.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{lst} \textendash{} list with words

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Extracting ingredients list:}
\label{\detokenize{api:extracting-ingredients-list}}\label{\detokenize{api:module-ingredient_extractor}}\index{ingredient\_extractor (module)}\index{IngredientExtractor (class in ingredient\_extractor)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{ingredient\_extractor.}}\sphinxbfcode{\sphinxupquote{IngredientExtractor}}}{\emph{config}, \emph{filename='models/crf\_vocab\_stuff\_europarl\_complete\_count.pkl'}}{}
Statistical Word Model to extract ingredients lists correctly.

A conditional random field is used to determine the correct
borders depending on the probabilities given by the statistical
word model.
\index{vocabulary (ingredient\_extractor.IngredientExtractor attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.vocabulary}}\pysigline{\sphinxbfcode{\sphinxupquote{vocabulary}}}
dictionary, mapping all known ingredients to their
frequence

\end{fulllineitems}

\index{trigram\_model (ingredient\_extractor.IngredientExtractor attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.trigram_model}}\pysigline{\sphinxbfcode{\sphinxupquote{trigram\_model}}}
dictionary, mapping word-triples to their number
of occurence at the end of seen ingredients lists

\end{fulllineitems}

\index{base\_model (ingredient\_extractor.IngredientExtractor attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.base_model}}\pysigline{\sphinxbfcode{\sphinxupquote{base\_model}}}
dictionary, mapping words from european parliament
speeches to their frequence

\end{fulllineitems}

\index{crf (ingredient\_extractor.IngredientExtractor attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.crf}}\pysigline{\sphinxbfcode{\sphinxupquote{crf}}}
conditional random field to determine the correct borders

\end{fulllineitems}

\index{whitelist (ingredient\_extractor.IngredientExtractor attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.whitelist}}\pysigline{\sphinxbfcode{\sphinxupquote{whitelist}}}
a set with known and allowed ingredients

\end{fulllineitems}

\index{blacklist (ingredient\_extractor.IngredientExtractor attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.blacklist}}\pysigline{\sphinxbfcode{\sphinxupquote{blacklist}}}
a set with known and prohibited ingredients

\end{fulllineitems}

\index{zutaten\_pat (ingredient\_extractor.IngredientExtractor attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.zutaten_pat}}\pysigline{\sphinxbfcode{\sphinxupquote{zutaten\_pat}}}
regular expression to match the beginning of an
ingredients list

\end{fulllineitems}

\index{token\_pattern\_just\_words (ingredient\_extractor.IngredientExtractor attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.token_pattern_just_words}}\pysigline{\sphinxbfcode{\sphinxupquote{token\_pattern\_just\_words}}}
regular expression to tokenize all
words with more than 2 characters and discard the rest

\end{fulllineitems}

\index{\_\_init\_\_() (ingredient\_extractor.IngredientExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{config}, \emph{filename='models/crf\_vocab\_stuff\_europarl\_complete\_count.pkl'}}{}
Initialize the models.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{config} \textendash{} dictionary with important configuration information

\item[{Keyword Arguments}] \leavevmode
\sphinxstylestrong{filename} \textendash{} path to a pickled file containing the trained
statistical word model and the conditional random field
(default: models/crf\_vocab\_stuff\_europarl\_complete\_count.pkl)

\end{description}\end{quote}

\end{fulllineitems}

\index{blacklist\_check() (ingredient\_extractor.IngredientExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.blacklist_check}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{blacklist\_check}}}{\emph{lst}}{}
Check if an ingredient from lst is prohibited.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{lst} \textendash{} ingredient list

\end{description}\end{quote}

\end{fulllineitems}

\index{create\_feat\_lists() (ingredient\_extractor.IngredientExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.create_feat_lists}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{create\_feat\_lists}}}{\emph{tokens}}{}
Create lists with probabilities for each token.

For each token the probability to be in an ingredient list,
not to be in an ingredient list and to be at the end of an
ingredient list is calculated. For convenience, three lists
are returned containing all the probabilities to be in an
ingredient list, not to be in an ingredient list and to be at
the end of an ingredient list.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{tokens} \textendash{} a list of tokens to evaluate

\end{description}\end{quote}

\end{fulllineitems}

\index{create\_vec\_list() (ingredient\_extractor.IngredientExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.create_vec_list}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{create\_vec\_list}}}{\emph{tokens}}{}
Create a list of vectors for the crf for tokens.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{tokens} \textendash{} a list of tokens to evaluate

\end{description}\end{quote}

\end{fulllineitems}

\index{extract() (ingredient\_extractor.IngredientExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.extract}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{extract}}}{\emph{text}}{}
Extract the ingredient list and calculate an occurence value.

The occurence value is an estimate for how often an ingredient
list with the same ending has been seen previously.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{text} \textendash{} text to extract the ingredient list from

\end{description}\end{quote}

\end{fulllineitems}

\index{extract\_zutaten() (ingredient\_extractor.IngredientExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.extract_zutaten}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{extract\_zutaten}}}{\emph{text}}{}
Extract tokens from all possible ingredients list.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{text} \textendash{} text to extract from

\end{description}\end{quote}

\end{fulllineitems}

\index{prob\_end() (ingredient\_extractor.IngredientExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.prob_end}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{prob\_end}}}{\emph{w2}, \emph{w1}, \emph{w}}{}
Calculate probability for words to be at the end.

To obtain a better estimate of the probability for a word,
this function should be used three times for each word. The
following setups should be used to evaluate the last word,
given a list of tokens l: prob\_end(l{[}-3{]}, l{[}-2{]}, l{[}-1{]}),
prob\_end(l{[}-2{]}, l{[}-1{]}, None) and prob\_end(l{[}-1{]}, None, None).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{w2} \textendash{} word preceding the word to evaluate by 2

\item {} 
\sphinxstylestrong{w1} \textendash{} word preceding the word to evaluate by 1

\item {} 
\sphinxstylestrong{w} \textendash{} word to evaluate

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{prob\_lm() (ingredient\_extractor.IngredientExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.prob_lm}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{prob\_lm}}}{\emph{w}}{}
Calculate probability for a word to be in an ingredients list.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{w} \textendash{} word to evaluate

\end{description}\end{quote}

\end{fulllineitems}

\index{prob\_nlm() (ingredient\_extractor.IngredientExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.prob_nlm}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{prob\_nlm}}}{\emph{w}}{}
Calculate probability for a word not to be in an ingredients list.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{w} \textendash{} word to evaluate

\end{description}\end{quote}

\end{fulllineitems}

\index{tokenize\_words() (ingredient\_extractor.IngredientExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.tokenize_words}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{tokenize\_words}}}{\emph{t}}{}
Tokenize a text.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{t} \textendash{} text to tokenize

\end{description}\end{quote}

\end{fulllineitems}

\index{whitelist\_check() (ingredient\_extractor.IngredientExtractor method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:ingredient_extractor.IngredientExtractor.whitelist_check}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{whitelist\_check}}}{\emph{lst}}{}
Check if an ingredient from lst is not known.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{lst} \textendash{} ingredient list

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Reconciliation with BioC information:}
\label{\detokenize{api:reconciliation-with-bioc-information}}\label{\detokenize{api:module-bioc}}\index{bioc (module)}\index{BiocStore (class in bioc)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:bioc.BiocStore}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{bioc.}}\sphinxbfcode{\sphinxupquote{BiocStore}}}
Functionality to check certificates from an offline storage.

The information about certificates is taken from the BioC
database. Therefore all the certificates where downloaded and
preprocessed in january 2018.
\index{oeko\_re (bioc.BiocStore attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:bioc.BiocStore.oeko_re}}\pysigline{\sphinxbfcode{\sphinxupquote{oeko\_re}}}
regular expression to extract the german ‘Ökonummer’

\end{fulllineitems}

\index{filename (bioc.BiocStore attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:bioc.BiocStore.filename}}\pysigline{\sphinxbfcode{\sphinxupquote{filename}}}
path to a pickled bioc\_store

\end{fulllineitems}

\index{info\_dict (bioc.BiocStore attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:bioc.BiocStore.info_dict}}\pysigline{\sphinxbfcode{\sphinxupquote{info\_dict}}}
dictionary mapping unique ids to certificate
information

\end{fulllineitems}

\index{mapping (bioc.BiocStore attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:bioc.BiocStore.mapping}}\pysigline{\sphinxbfcode{\sphinxupquote{mapping}}}
dictionary mapping all normalised addresses to ids used
in info\_dict

\end{fulllineitems}

\index{\_\_init\_\_() (bioc.BiocStore method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:bioc.BiocStore.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{}{}
Initialize the store.

\end{fulllineitems}

\index{build\_dict() (bioc.BiocStore method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:bioc.BiocStore.build_dict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{build\_dict}}}{\emph{directory}}{}
Build dictionaries for certificates and store them.

The certificates should be stored in a directory with
subdirectories. Each subdirectory contains websites with
certificate information, while directory only contains
subdirectories.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{directory} \textendash{} path to a directory with a given structure.

\end{description}\end{quote}

\end{fulllineitems}

\index{check\_validity() (bioc.BiocStore method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:bioc.BiocStore.check_validity}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{check\_validity}}}{\emph{valids}}{}
Check the validity of a certificate.

TODO: this is outdated and needs to be refactored
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{valids} \textendash{} list with valid periods

\end{description}\end{quote}

\end{fulllineitems}

\index{extract\_all() (bioc.BiocStore method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:bioc.BiocStore.extract_all}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{extract\_all}}}{\emph{filename}}{}
Extract all certificate information from a website.

For a certificate the following information is extracted and
normalized: all addresses, all periods where the certificate
is valid, the responsible ‘Ökokontrollstellen’.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{filename} \textendash{} path to a website, could also be an url.

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_certificate\_for\_addresses() (bioc.BiocStore method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:bioc.BiocStore.get_certificate_for_addresses}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_certificate\_for\_addresses}}}{\emph{addresses}}{}
Get certificate information from the store for some addresses.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{addresses} \textendash{} list of normalised addresses

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Utils:}
\label{\detokenize{api:module-my_exceptions}}\label{\detokenize{api:utils}}\index{my\_exceptions (module)}\index{TooLongException}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:my_exceptions.TooLongException}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{exception }}\sphinxcode{\sphinxupquote{my\_exceptions.}}\sphinxbfcode{\sphinxupquote{TooLongException}}}{\emph{value}}{}
Exception, when a module takes to long.
\index{value (my\_exceptions.TooLongException attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:my_exceptions.TooLongException.value}}\pysigline{\sphinxbfcode{\sphinxupquote{value}}}
message of the exception

\end{fulllineitems}

\index{\_\_init\_\_() (my\_exceptions.TooLongException method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:my_exceptions.TooLongException.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{value}}{}
Initialize the exception.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{value} \textendash{} message of the exception

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{api:module-simple_check}}\index{simple\_check (module)}\index{SimpleCheck (class in simple\_check)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:simple_check.SimpleCheck}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{simple\_check.}}\sphinxbfcode{\sphinxupquote{SimpleCheck}}}
Wrapper for approximative string matching.
\index{matchers\_lst (simple\_check.SimpleCheck attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:simple_check.SimpleCheck.matchers_lst}}\pysigline{\sphinxbfcode{\sphinxupquote{matchers\_lst}}}
list of lists of different matchers that are
used in every check method

\end{fulllineitems}

\index{words\_lst (simple\_check.SimpleCheck attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:simple_check.SimpleCheck.words_lst}}\pysigline{\sphinxbfcode{\sphinxupquote{words\_lst}}}
list of lists of the corresponding search terms

\end{fulllineitems}

\index{\_\_init\_\_() (simple\_check.SimpleCheck method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:simple_check.SimpleCheck.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{}{}
Initialize the list of matchers and words as empty.

\end{fulllineitems}

\index{add\_list() (simple\_check.SimpleCheck method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:simple_check.SimpleCheck.add_list}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{add\_list}}}{\emph{lst}}{}
Add a matcher for every word in lst.

Adds a list of words that are searched for in the texts. The
list should contain a word with multiple synomymous words.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{lst} \textendash{} list of words that are added

\end{description}\end{quote}

\end{fulllineitems}

\index{lst\_distance() (simple\_check.SimpleCheck method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:simple_check.SimpleCheck.lst_distance}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{lst\_distance}}}{\emph{lst}, \emph{distance}}{}
Filter the matches within a given distance.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{lst} \textendash{} list with matching results

\item {} 
\sphinxstylestrong{distance} \textendash{} allowed distance in characters

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{range\_check\_text() (simple\_check.SimpleCheck method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:simple_check.SimpleCheck.range_check_text}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{range\_check\_text}}}{\emph{text}, \emph{distance}, \emph{k=0}}{}
Check for search terms within a given distance.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{text} \textendash{} text to be searched

\item {} 
\sphinxstylestrong{distance} \textendash{} allowed distance of matches in characters

\end{itemize}

\item[{Keyword Arguments}] \leavevmode
\sphinxstylestrong{k} \textendash{} number of acceptable errors (default: 0)

\end{description}\end{quote}

\end{fulllineitems}

\index{simple\_check\_text() (simple\_check.SimpleCheck method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:simple_check.SimpleCheck.simple_check_text}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{simple\_check\_text}}}{\emph{text}, \emph{k=0}}{}
Check the text for occurences of terms.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{text} \textendash{} text that should be searched

\item[{Keyword Arguments}] \leavevmode
\sphinxstylestrong{k} \textendash{} number of acceptable errors (default: 0)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{api:module-approx_str_matching}}\index{approx\_str\_matching (module)}\index{Matcher (class in approx\_str\_matching)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:approx_str_matching.Matcher}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{approx\_str\_matching.}}\sphinxbfcode{\sphinxupquote{Matcher}}}{\emph{pattern}}{}
This class implements a fast text searching algorithm allowing for
errors. This Algorithm is implemented after {[}0{]}. For every Pattern
that should be searched in a text, there has to be a little
preprocessing. So the Matcher Objects are for one pattern
each. The Matcher needs the pattern upon its initialization, so
the pattern has to be passed to the constructor.
\index{s (approx\_str\_matching.Matcher attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:approx_str_matching.Matcher.s}}\pysigline{\sphinxbfcode{\sphinxupquote{s}}}
default dictionary, that contains a bitvector for each
character in the pattern. The bitvecor has the length of the
pattern and contains a 1 at every position the corresponding
character is in the pattern.

\end{fulllineitems}

\index{m (approx\_str\_matching.Matcher attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:approx_str_matching.Matcher.m}}\pysigline{\sphinxbfcode{\sphinxupquote{m}}}
length of the pattern.

\end{fulllineitems}


\begin{sphinxadmonition}{note}{Note:}\begin{description}
\item[{{[}0{]}: Sun Wu and Udi Manber. 1992. Fast text searching: allowing}] \leavevmode
errors. Commun. ACM 35, 10 (October 1992), 83-91.

\end{description}
\end{sphinxadmonition}
\index{\_\_init\_\_() (approx\_str\_matching.Matcher method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:approx_str_matching.Matcher.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{pattern}}{}
Initialize the Matcher Object for a given pattern.

Constructs the default dictionary \sphinxcode{\sphinxupquote{self.s}} and inserts the
bitvectors for every character in pattern.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{pattern} \textendash{} pattern, that should be searched in the text.

\end{description}\end{quote}

\end{fulllineitems}

\index{match\_approx() (approx\_str\_matching.Matcher method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:approx_str_matching.Matcher.match_approx}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{match\_approx}}}{\emph{text}, \emph{k}}{}
Search the corresponding pattern in text with max k errors.

Only the match with the lowest error rate for one position of
the text is reported. Returns a list with the ending positions
in text of the matches.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{text} \textendash{} the text to be searched.

\item {} 
\sphinxstylestrong{k} \textendash{} number of allowed errors

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{match\_exact() (approx\_str\_matching.Matcher method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:approx_str_matching.Matcher.match_exact}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{match\_exact}}}{\emph{text}}{}
Search the corresponding pattern in text.

Returns a list with the ending positions in text of the
matches.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{text} \textendash{} the text to be searched.

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{a}
\item {\sphinxstyleindexentry{anr\_matcher}}\sphinxstyleindexpageref{api:\detokenize{module-anr_matcher}}
\item {\sphinxstyleindexentry{approx\_str\_matching}}\sphinxstyleindexpageref{api:\detokenize{module-approx_str_matching}}
\indexspace
\bigletter{b}
\item {\sphinxstyleindexentry{backend}}\sphinxstyleindexpageref{api:\detokenize{module-backend}}
\item {\sphinxstyleindexentry{bioc}}\sphinxstyleindexpageref{api:\detokenize{module-bioc}}
\indexspace
\bigletter{c}
\item {\sphinxstyleindexentry{classifier}}\sphinxstyleindexpageref{api:\detokenize{module-classifier}}
\item {\sphinxstyleindexentry{color\_filter2}}\sphinxstyleindexpageref{api:\detokenize{module-color_filter2}}
\item {\sphinxstyleindexentry{conll\_parser}}\sphinxstyleindexpageref{api:\detokenize{module-conll_parser}}
\indexspace
\bigletter{d}
\item {\sphinxstyleindexentry{db\_interface}}\sphinxstyleindexpageref{api:\detokenize{module-db_interface}}
\indexspace
\bigletter{f}
\item {\sphinxstyleindexentry{find\_logos}}\sphinxstyleindexpageref{api:\detokenize{module-find_logos}}
\indexspace
\bigletter{g}
\item {\sphinxstyleindexentry{geoschutz\_check}}\sphinxstyleindexpageref{api:\detokenize{module-geoschutz_check}}
\indexspace
\bigletter{h}
\item {\sphinxstyleindexentry{health\_claims}}\sphinxstyleindexpageref{api:\detokenize{module-health_claims}}
\indexspace
\bigletter{i}
\item {\sphinxstyleindexentry{impressum}}\sphinxstyleindexpageref{api:\detokenize{module-impressum}}
\item {\sphinxstyleindexentry{impressum\_crf}}\sphinxstyleindexpageref{api:\detokenize{module-impressum_crf}}
\item {\sphinxstyleindexentry{information\_extractor}}\sphinxstyleindexpageref{api:\detokenize{module-information_extractor}}
\item {\sphinxstyleindexentry{ingredient\_extractor}}\sphinxstyleindexpageref{api:\detokenize{module-ingredient_extractor}}
\indexspace
\bigletter{l}
\item {\sphinxstyleindexentry{logos}}\sphinxstyleindexpageref{api:\detokenize{module-logos}}
\indexspace
\bigletter{m}
\item {\sphinxstyleindexentry{my\_exceptions}}\sphinxstyleindexpageref{api:\detokenize{module-my_exceptions}}
\indexspace
\bigletter{o}
\item {\sphinxstyleindexentry{oeko}}\sphinxstyleindexpageref{api:\detokenize{module-oeko}}
\indexspace
\bigletter{p}
\item {\sphinxstyleindexentry{pos\_tagger}}\sphinxstyleindexpageref{api:\detokenize{module-pos_tagger}}
\item {\sphinxstyleindexentry{preprocess}}\sphinxstyleindexpageref{api:\detokenize{module-preprocess}}
\item {\sphinxstyleindexentry{probability}}\sphinxstyleindexpageref{api:\detokenize{module-probability}}
\item {\sphinxstyleindexentry{product\_crf}}\sphinxstyleindexpageref{api:\detokenize{module-product_crf}}
\indexspace
\bigletter{s}
\item {\sphinxstyleindexentry{simple\_check}}\sphinxstyleindexpageref{api:\detokenize{module-simple_check}}
\indexspace
\bigletter{w}
\item {\sphinxstyleindexentry{worker}}\sphinxstyleindexpageref{api:\detokenize{module-worker}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}